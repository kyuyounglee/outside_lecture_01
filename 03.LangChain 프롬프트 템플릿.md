# LangChain 프롬프트 템플릿

## 소개

LangChain의 **프롬프트 템플릿(PromptTemplate)** 기능은 LLM 프롬프트를 동적으로 구성하고 재사용할 수 있게 해주는 도구입니다. 본 실습 자료에서는 LangChain v0.3.x 환경에서 OpenAI의 GPT-4o-mini 모델을 이용하여 프롬프트 템플릿의 다양한 기능을 살펴보겠습니다. Jupyter 노트북이나 VS Code 등 인터랙티브 환경에서 따라 할 수 있도록 코드 예제와 출력 결과를 함께 제공합니다. OpenAI API 키 등 민감한 정보는 `.env` 파일에 저장하고 **python-dotenv**로 불러오는 방식으로 관리합니다.

이 자료에서 다룰 주요 내용은 다음과 같습니다:

* **단일 입력 PromptTemplate**: 하나의 변수로 구성된 프롬프트 템플릿 개념과 예시 (예: 제품 설명 문구 생성)
* **다중 입력 PromptTemplate**: 둘 이상의 변수를 사용하는 템플릿 예시 (예: 제목과 키워드로 요약문 생성)
* **ChatPromptTemplate와 역할 기반 프롬프트**: 시스템/사용자 역할별 프롬프트 구성과 `ChatPromptTemplate.from_messages()` 사용법
* **PartialPromptTemplate 활용**: 프롬프트 일부를 미리 고정하는 부분 포맷팅 개념과 실용 예 (예: 시스템 메시지는 고정하고 사용자 입력만 변경)
* **프롬프트 출력 및 체인 실행**: 완성된 프롬프트를 확인하고 LLM과 연결하여 실행하는 방법 (LangChain Expression Language 기반 `prompt | llm` 파이프라인 사용)
* **프롬프트 작성 팁**: 프롬프트 템플릿 작성 시 주의할 사항과 실무적인 모범 사례

**Note:** 실습에 사용되는 모든 프롬프트 문장은 한국어로 작성되어, 모델이 한국어로 출력하거나 맥락에 맞게 응답하도록 합니다.

## 환경 설정 (Installation & Setup)

본 실습을 시작하기 전에, 필요한 패키지와 API 키를 설정해야 합니다.

1. **필요 패키지 설치**: LangChain과 OpenAI 연동, dotenv를 설치합니다. 터미널이나 Jupyter 노트북 코드 셀에서 아래 명령을 실행하세요:

```bash
pip install langchain langchain-openai python-dotenv
```

2. **OpenAI API 키 설정**: OpenAI GPT-4o-mini 모델을 호출하려면 OpenAI API 키가 필요합니다. API 키를 안전하게 관리하기 위해 프로젝트 루트에 `.env` 파일을 만들고 다음과 같이 키를 저장합니다 (여기서는 키 형식 예시만 표시합니다):

```text
# .env 파일 내용 (예시)
OPENAI_API_KEY=sk-********************
```

`.env` 파일은 버전 관리에서 제외(.gitignore 등)하여 키가 노출되지 않도록 합니다.

3. **환경 변수 로드**: Python 코드에서 `python-dotenv`를 이용해 `.env` 파일의 내용을 불러옵니다. 예를 들어, Jupyter 노트북에서는 다음과 같이 환경 변수를 설정할 수 있습니다:

```python
from dotenv import load_dotenv
load_dotenv()  # .env 파일 로드
import os
openai_key = os.getenv("OPENAI_API_KEY")
print("API Key Loaded:", openai_key is not None)
```

위 코드에서는 `.env` 파일의 `OPENAI_API_KEY`를 읽어와 `openai_key` 변수에 저장합니다. LangChain의 OpenAI 연동 클래스는 기본적으로 환경 변수에 설정된 API 키를 자동으로 사용하므로, 별도로 키를 코드에 하드코딩할 필요가 없습니다.

이제 환경 설정이 완료되었습니다. 다음 섹션부터 LangChain의 프롬프트 템플릿을 활용한 다양한 예제를 실습해보겠습니다.

## 1. 단일 입력 PromptTemplate

**PromptTemplate**은 하나 이상의 변수에 기반하여 프롬프트 문자열을 생성하는 템플릿 클래스입니다. 사용자 입력을 가공하여 LLM에 전달할 최종 프롬프트를 만들어 주며, 이를 통해 프롬프트 작성의 일관성과 재사용성을 높일 수 있습니다. 프롬프트 템플릿을 사용하면 매번 전체 프롬프트를 새로 쓰지 않고도 필요한 부분만 바꾸어 일관된 형식의 프롬프트를 생성할 수 있습니다.

### 개념 설명

단일 입력 PromptTemplate은 하나의 `input_variable`을 받아들이는 가장 기본적인 형태의 프롬프트 템플릿입니다. 예를 들어 **제품 설명**을 생성하는 간단한 프롬프트를 생각해봅시다. 사용자가 **제품명**만 제공하면, 미리 준비된 템플릿에 제품명을 삽입하여 모델에게 마케팅 용도의 제품 설명을 요청할 수 있습니다.

LangChain에서 PromptTemplate를 사용하는 기본 흐름은 아래와 같습니다:

1. **템플릿 문자열 정의**: 프롬프트의 골격이 되는 문자열을 작성합니다. 이 문자열에는 중괄호 `{}`로 둘러싼 플레이스홀더(변수 이름)를 포함할 수 있습니다.
2. **PromptTemplate 객체 생성**: 템플릿 문자열과 입력 변수명을 지정하여 PromptTemplate 객체를 만듭니다.
3. **포맷(format) 수행**: `PromptTemplate.format()` 메서드 등에 입력값을 주어 실제 프롬프트 문자열을 생성합니다.
4. **LLM 호출**: 완성된 프롬프트 문자열을 LLM에 전달하여 결과를 얻습니다.

### 예시: 제품 설명 프롬프트

아래 예제에서는 단일 변수 `product_name`을 받아 해당 제품의 홍보 문구를 생성하는 프롬프트 템플릿을 만들어보겠습니다. 프롬프트 내용은 한국어로 작성되며, 모델에게 제품의 **특징과 장점**을 강조한 마케팅 문구 작성을 요청합니다. (100자 내외의 길이 제한을 두어 간결한 문구를 얻도록 지시합니다.)

```python
from langchain.prompts import PromptTemplate

# 1. 템플릿 문자열 정의 (한국어로 작성)
template_str = (
    "당신은 최고 수준의 마케팅 카피라이터입니다.\n"
    "아래 제품의 매력적인 홍보 문구를 100자 내외로 작성해주세요.\n\n"
    "제품명: {product_name}"
)

# 2. PromptTemplate 객체 생성
product_prompt = PromptTemplate.from_template(template_str)

# 3. 템플릿에 값 채워보기 (format)
formatted_prompt = product_prompt.format(product_name="슈퍼카메라 X100")
print(formatted_prompt)
```

위 코드에서 `PromptTemplate.from_template` 메서드를 사용하면 템플릿 문자열에 포함된 `{product_name}` 변수를 자동으로 인식하여 `input_variables=["product_name"]`인 PromptTemplate 객체를 반환합니다. `formatted_prompt`에는 실제 제품명 "슈퍼카메라 X100"이 삽입된 최종 프롬프트 문자열이 들어갑니다. `print` 함수를 통해 이 결과를 확인하면 다음과 같은 출력이 나타납니다:

```text
당신은 최고 수준의 마케팅 카피라이터입니다.
아래 제품의 매력적인 홍보 문구를 100자 내외로 작성해주세요.

제품명: 슈퍼카메라 X100
```

이처럼 `PromptTemplate.format`을 사용하여 사용자 입력이 반영된 프롬프트 문자열을 쉽게 생성할 수 있습니다. **PromptTemplate**은 내부적으로 f-string과 유사한 포맷팅을 수행하며, 정의된 변수 이름들과 실제 값을 매핑해 줍니다.

### LLM 실행 및 출력 예시

이제 완성된 프롬프트 `formatted_prompt`를 실제 LLM에 보내서 결과를 받아보겠습니다. LangChain에서는 LLM 호출을 위해 `langchain.llms` 모듈의 OpenAI 기반 클래스나 `langchain.chat_models` 모듈의 ChatOpenAI 클래스를 사용할 수 있습니다. 여기서는 OpenAI의 GPT-4o-mini 모델을 사용하기 위해 ChatOpenAI 클래스를 활용하겠습니다.

**Tip:** PromptTemplate와 LLM을 연결하여 실행할 때는 LangChain Expression Language(LCEL)의 **파이프라인(`|`) 기능**을 사용할 수 있습니다. 파이프 연산자는 왼쪽의 출력(프롬프트)을 오른쪽의 입력(LLM)으로 바로 전달해주며, 이를 통해 체인을 간결하게 표현할 수 있습니다. (본 자료에서는 편의를 위해 이 파이프라인 기법을 사용하며, 자세한 설명은 뒤의 섹션에서 다룹니다.)

```python
from langchain_openai import ChatOpenAI

# OpenAI GPT-4o-mini 모델 초기화 (temperature=0으로 설정하여 출력 안정성 강화)
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# RunnableSequence 객체 생성 (파이프라인 형태)
chain = product_prompt | llm

# invoke 메서드를 사용하여 명시적으로 호출
result = chain.invoke({"product_name": "슈퍼카메라 X100"})
print(result)
```

위 코드에서 `(product_prompt | llm)( {"product_name": "슈퍼카메라 X100"} )` 표현은 `product_prompt.format(product_name="...")`로 프롬프트를 생성한 뒤, 곧바로 `llm`에 전달하여 결과를 얻는 과정을 한 번에 수행합니다.

실제로 GPT-4o-mini에 해당 프롬프트를 전달하면 아래와 같은 홍보 문구 예시를 얻을 수 있습니다:

```text
슈퍼카메라 X100 – 초고화질로 순간을 포착하고 AI 자동 촬영으로 누구나 전문가처럼 찍을 수 있는 차세대 카메라!
```

위 **예시 출력**은 모델이 생성한 제품 홍보 문구입니다. 제품의 특징(초고화질, AI 자동 촬영)과 혜택(누구나 전문가처럼 촬영)을 100자 이내로 잘 요약하고 있습니다. 프롬프트에 제시한 요구사항에 맞게 응답이 나오는지 확인하는 것이 중요합니다.

이제 단일 입력 프롬프트 템플릿의 개념을 이해했으니, 여러 개의 입력 변수를 사용하는 경우를 살펴보겠습니다.

## 2. 다중 입력 PromptTemplate

하나의 프롬프트 템플릿에서 **두 개 이상**의 변수를 사용할 수도 있습니다. 이를 통해 보다 복잡한 요청을 동적으로 구성할 수 있습니다. 예를 들어, **기사 요약 생성** 시나리오를 생각해보겠습니다. 사용자가 기사 **제목**과 주요 **키워드**를 제공하면, 이를 기반으로 기사의 핵심 내용을 요약하도록 프롬프트를 구성할 수 있습니다.

### 개념 설명

다중 입력 PromptTemplate은 `input_variables` 리스트에 여러 변수명을 포함하며, 각 변수에 대응하는 값을 format 시에 모두 전달해야 합니다. 템플릿 문자열 내에 `{변수명}` 형태로 여러 번 삽입된 플레이스홀더들은 각각 대응되는 값으로 치환됩니다.

요약 생성 예시의 경우:

* `title`: 기사 제목
* `keywords`: 주요 키워드 (콤마로 구분된 문자열 등)

이 두 가지를 입력으로 받아 템플릿에 넣은 뒤, 모델에게 해당 정보를 토대로 요약문을 생성하도록 요청할 것입니다.

### 예시: 제목과 키워드를 활용한 요약 프롬프트

아래 코드는 `title`과 `keywords`라는 두 변수를 받아들이는 PromptTemplate을 정의하고, 샘플 값을 대입하여 포맷팅한 결과를 보여줍니다.

```python
# 1. 다중 입력 템플릿 문자열 정의
multi_template_str = (
    "아래는 뉴스 기사 제목과 키워드입니다.\n"
    "이 정보를 바탕으로 한 문단으로 구성된 간략한 요약문을 작성하세요.\n\n"
    "제목: {title}\n"
    "키워드: {keywords}"
)

# 2. PromptTemplate 객체 생성 (두 개의 input_variables 지정)
summary_prompt = PromptTemplate(
    template=multi_template_str, input_variables=["title", "keywords"]
)

# 3. 예시 값으로 포맷팅하여 프롬프트 확인
sample_title = "인공지능 기술의 발전과 미래"
sample_keywords = "머신러닝, 딥러닝, 산업 혁신"
formatted_summary_prompt = summary_prompt.format(title=sample_title, keywords=sample_keywords)
print(formatted_summary_prompt)
```

위 템플릿은 기사 제목과 키워드를 제시한 후, 한 문단으로 요약해달라고 모델에게 요청하는 내용입니다. 두 변수를 `.format()`에 전달하면, `formatted_summary_prompt`에 다음과 같은 문자열이 생성됩니다:

```text
아래는 뉴스 기사 제목과 키워드입니다.
이 정보를 바탕으로 한 문단으로 구성된 간략한 요약문을 작성하세요.

제목: 인공지능 기술의 발전과 미래
키워드: 머신러닝, 딥러닝, 산업 혁신
```

이처럼 다중 변수 템플릿을 사용하면 여러 입력을 한꺼번에 프롬프트에 반영할 수 있습니다. 이제 이 프롬프트를 LLM에 전달하여 실제 요약 결과를 얻어보겠습니다.

### LLM 실행 및 출력 예시

앞서 설정한 `llm` (ChatOpenAI with GPT-4o-mini)을 재사용하여, 위 요약 프롬프트에 대한 응답을 생성해보겠습니다. 마찬가지로 프롬프트 템플릿과 LLM을 파이프로 연결해 실행합니다.

```python
# 4. 프롬프트 템플릿과 LLM 실행 (파이프라인 사용)
result_summary = (summary_prompt | llm).invoke({
    "title": sample_title,
    "keywords": sample_keywords
})
print(result_summary)
```

모델로부터 생성된 요약문 예시는 다음과 같습니다:

```text
이 기사는 인공지능 기술이 머신러닝과 딥러닝을 중심으로 급격히 발전하여 다양한 산업 혁신을 이끌고 있으며, 이러한 기술 발전이 미래에 미칠 영향에 대해 다루고 있습니다.
```

**예시 출력**을 보면, 제공한 제목과 키워드(인공지능 기술, 머신러닝, 딥러닝, 산업 혁신)를 토대로 모델이 한 문단으로 뉴스 기사 요약을 작성했습니다. 프롬프트에서 요구한 대로 한 문단으로 구성되었으며, 주요 키워드를 포함하여 자연스러운 요약이 이루어진 것을 확인할 수 있습니다.

다중 입력 PromptTemplate을 사용하면 이처럼 여러 정보를 조합한 지시를 한 번에 모델에게 전달할 수 있습니다. 다음으로, OpenAI GPT-4와 같은 **채팅 기반 모델**을 위한 ChatPromptTemplate 사용법을 알아보겠습니다.

## 3. ChatPromptTemplate과 역할 기반 프롬프트

**ChatPromptTemplate**은 시스템/사용자/어시스턴트 등의 \*\*역할(role)\*\*을 포함한 다중 메시지 프롬프트를 구성하기 위한 템플릿 클래스입니다. GPT-4와 GPT-3.5 등 ChatCompletion 계열 모델은 대화 형식의 입력을 받으므로, 단순한 문자열보다 **메시지의 목록** (예: 시스템 메시지 + 사용자 메시지 등) 형태로 프롬프트를 구성해야 합니다. ChatPromptTemplate은 이러한 여러 역할의 메시지들을 하나의 프롬프트 템플릿으로 관리할 수 있게 해주며, LangChain에서는 `ChatPromptTemplate.from_messages()` 메서드를 통해 다양한 형식의 메시지를 조합할 수 있습니다.

### 개념 설명

ChatPromptTemplate에서는 **시스템 메시지**, **사용자 메시지**, (선택적으로 **어시스턴트 메시지**) 등을 순서대로 정의하여 컨텍스트가 있는 프롬프트를 만들 수 있습니다. 일반적으로:

* **시스템 메시지**: 모델의 동작을 지시하는 역할(예: 모델의 성격, 태도, 임무 설명).
* **사용자 메시지**: 실제 사용자 입력에 해당하는 질문이나 요청.
* **어시스턴트 메시지**: 이전에 모델(assistant)이 응답한 내용이 있다면 대화 맥락으로 포함 (few-shot 예시 제공 등에 활용).

ChatPromptTemplate을 구성할 때 각 메시지에 플레이스홀더를 넣어두고, 최종 실행 시 해당 값들을 채워넣는 방식은 PromptTemplate과 유사합니다. 다만 출력 결과는 여러 메시지 객체들의 모음으로 나타나며, LangChain이 이를 Chat 모델 호출에 적합한 형식으로 전달합니다.

### 예시: 역할 기반 Q\&A 프롬프트

다음 예에서는 시스템 메시지와 사용자 메시지 두 부분으로 구성된 ChatPromptTemplate을 만들어보겠습니다. 시스템 역할은 모델에게 특정 전문가로서 답변하도록 지시하고, 사용자 메시지에는 실제 질문을 넣는 형태입니다. (역할 지시와 질문 모두 한국어로 작성합니다.)

시나리오: **Python 프로그래밍 전문가** 역할을 부여하고, 사용자가 Python 관련 질문을 하면 친절하고 간결한 답변을 하는 상황.

```python
from langchain.prompts import ChatPromptTemplate

# 1. 시스템 및 사용자 템플릿 정의
system_message = ("당신은 Python 분야의 뛰어난 전문가이자 조언자입니다. "
                  "사용자의 프로그래밍 질문에 대해 친절하고 이해하기 쉽게 답변해주세요.")
user_message = "{question}"  # 사용자 질문은 실행 시 채워질 변수

# 2. ChatPromptTemplate.from_messages를 사용하여 역할별 프롬프트 템플릿 생성
chat_prompt = ChatPromptTemplate.from_messages([
    ("system", system_message),
    ("user", user_message)
])
```

위 코드에서 `ChatPromptTemplate.from_messages`에 전달된 리스트에는 `(role, content)` 형태의 튜플이 사용되었습니다. `"system"` 역할에는 고정된 지시(system\_message 내용)가 들어가고, `"user"` 역할에는 `{question}` 플레이스홀더가 포함된 사용자 메시지 템플릿이 들어갑니다. 이렇게 하면 이 템플릿은 `question` 변수 하나를 입력으로 받아, 두 개의 메시지(시스템 + 사용자)를 갖춘 프롬프트를 생성하게 됩니다.

이제 `question` 변수에 실제 질문을 넣어 프롬프트를 형성해보겠습니다. `ChatPromptTemplate`에서는 `.format_messages()` 메서드를 사용하여 메시지 객체들의 리스트를 얻을 수 있습니다.

```python
# 3. format_messages로 예시 질문을 채워 메시지 목록 생성
sample_question = "파이썬의 리스트 컴프리헨션(list comprehension)과 map 함수의 차이가 무엇인가요?"
messages = chat_prompt.format_messages(question=sample_question)
print(messages)
```

`sample_question`에 담긴 질문을 포맷에 넣어주면, `messages` 변수는 시스템 메시지와 사용자 메시지를 포함하는 Python 객체 리스트가 됩니다. 위 `print(messages)`를 통해 확인하면 대략 다음과 같은 출력 (메시지 객체들의 표현)으로 나타납니다:

```text
[SystemMessage(content='당신은 Python 분야의 뛰어난 전문가이자 조언자입니다. 사용자의 프로그래밍 질문에 대해 친절하고 이해하기 쉽게 답변해주세요.', additional_kwargs={}, response_metadata={}),
 HumanMessage(content='파이썬의 리스트 컴프리헨션(list comprehension)과 map 함수의 차이가 무엇인가요?', additional_kwargs={}, response_metadata={})]
```

위 출력에서 **SystemMessage**와 **HumanMessage** 두 객체가 리스트로 포함되어 있음을 볼 수 있습니다. `content` 필드에 각각 시스템 지시와 사용자 질문이 잘 채워져 있습니다. LangChain의 ChatPromptTemplate은 이러한 메시지 목록(`ChatPromptValue`)을 생성하여 Chat 모델이 이해할 수 있는 구조로 반환합니다.

이제 이 메시지들을 GPT-4o-mini 모델에 보내서 답변을 받아보겠습니다.

```python
# 4. Chat 모델 실행하여 응답 얻기
answer = (chat_prompt | llm).invoke({"question": sample_question})
print(answer.content)
```

프롬프트 템플릿 `chat_prompt`와 LLM `llm`을 파이프로 연결한 뒤 `question` 값을 넣어 실행하면, `answer`로 LangChain의 **AIMessage** 객체가 반환됩니다. `answer.content`를 출력하면 모델의 답변 텍스트를 볼 수 있습니다. 예시 응답은 아래와 같습니다:

```text
리스트 컴프리헨션은 표현식 기반으로 리스트를 간결하게 생성하는 구문이고, 가독성이 좋습니다. 반면 `map` 함수는 함수를 이용해 기존 리스트의 각 요소를 변환하지만, 결과를 리스트로 얻으려면 `list()`로 감싸야 합니다. 일반적으로 간단한 변환에는 리스트 컴프리헨션을 쓰고, 복잡한 변환이나 재사용할 함수가 있을 때는 `map` 함수를 사용합니다.
```

모델은 Python 전문가로서 **리스트 컴프리헨션**과 **map 함수**의 차이를 이해하기 쉽게 설명했습니다. 시스템 지시에 따라 친절하고 자세한 어조를 유지한 답변을 생성한 것을 확인할 수 있습니다. ChatPromptTemplate을 사용함으로써 이러한 역할 지시와 사용자 질문을 하나의 템플릿으로 관리하고, 매번 다른 질문에 대해 일관된 형식의 대화를 생성할 수 있습니다.

지금까지 단일/다중 입력 PromptTemplate과 ChatPromptTemplate의 기본 사용법을 살펴보았습니다. 다음으로, **PartialPromptTemplate**을 사용하여 프롬프트의 일부만 미리 채워두고 나머지를 나중에 입력받는 방법을 알아보겠습니다.

## 4. PartialPromptTemplate 활용

어떤 경우에는 프롬프트 템플릿의 일부 변수는 항상 고정되거나 미리 알 수 있고, 다른 일부 변수는 매 호출 시 달라지는 상황이 있습니다. 이런 경우 **PartialPromptTemplate** 기능을 통해 템플릿 일부를 **부분적으로 채운 새로운 템플릿**을 만들 수 있습니다. 쉽게 말해, 함수에 일부 인자를 미리 바인딩해두는 *부분 함수 적용*과 유사하게 프롬프트 템플릿에도 일부 변수를 미리 채워둘 수 있습니다.

PartialPromptTemplate을 사용하면 체인의 앞부분에서 얻은 값을 템플릿에 미리 주입한 후, 나중 단계에서 나머지 값만 넣어주면 되므로 편리합니다. 또한, 반복적으로 변하지 않는 정보(예: 시스템 역할 지시, 공통 맥락 등)를 매번 입력하지 않아도 되도록 설정할 수 있습니다.

### 개념 설명

LangChain에서는 PromptTemplate 객체의 `.partial()` 메서드를 사용하여 부분 포맷팅을 수행합니다. **partial** 메서드에는 미리 채우고자 하는 변수명을 키워드 인자 형태로 전달하면 됩니다. partial을 거친 템플릿은 남은 변수만을 요구하는 새로운 PromptTemplate로 취급되며, 이후에는 남은 변수들만 `.format()`으로 채워주면 완성됩니다.

ChatPromptTemplate 역시 partial을 지원하며, 시스템/사용자 메시지 중 일부에 들어갈 변수를 고정할 때 활용할 수 있습니다.

### 예시: 시스템 역할 고정 및 사용자 입력만 변경

이번 예제에서는 **시스템 메시지에 포함된 변수**를 partial로 고정하는 상황을 만들어 보겠습니다. 예컨대, 다양한 분야의 전문가로 답변하는 챗봇을 만든다고 가정하면, 시스템 메시지 템플릿에 `{role}`이라는 변수를 넣어두고 이 값을 partial을 통해 고정함으로써 해당 역할에 특화된 프롬프트를 손쉽게 생성할 수 있습니다. 이후에는 사용자 질문만 다르게 넣어서 여러 분야에 대한 QA를 구현할 수 있습니다.

아래 코드는 `{role}`과 `{question}` 두 변수를 갖는 ChatPromptTemplate을 정의한 뒤, `role`을 partial로 채워 **주식 투자 전문가** 역할에 고정한 프롬프트를 만드는 과정을 보여줍니다:

```python
from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate

# 1. 시스템/사용자용 BaseMessagePromptTemplate 생성
role_system_template = "당신은 {role} 분야의 전문 지식인입니다. 가능한 한 자세히 답변해 주세요."
system_prompt = SystemMessagePromptTemplate.from_template(role_system_template)
user_prompt = HumanMessagePromptTemplate.from_template("{question}")

# 2. ChatPromptTemplate 생성 (role과 question 변수를 사용)
base_chat_prompt = ChatPromptTemplate.from_messages([system_prompt, user_prompt])

# 3. PartialPromptTemplate: role 변수를 미리 채워 새로운 프롬프트 생성
partial_chat_prompt = base_chat_prompt.partial(role="주식 투자")
```

1단계에서 `SystemMessagePromptTemplate.from_template`과 `HumanMessagePromptTemplate.from_template`을 이용해 역할과 질문에 대한 하위 템플릿을 만들었습니다. 그런 다음 이를 리스트로 묶어 `ChatPromptTemplate.from_messages`로 종합 프롬프트를 생성했습니다. 이 템플릿 `base_chat_prompt`는 `role`과 `question` 두 변수를 필요로 합니다.

3단계에서는 `base_chat_prompt.partial(role="주식 투자")`를 호출하여 `role` 변수에 "주식 투자" 값을 미리 채운 새로운 ChatPromptTemplate `partial_chat_prompt`를 얻었습니다. 이제 이 `partial_chat_prompt`는 더 이상 `role` 값을 필요로 하지 않으며, 남은 변수 `question`만 제공하면 됩니다.

이 `partial_chat_prompt`의 동작을 확인하기 위해 `question`만 채워서 실제 메시지들을 만들어 보겠습니다:

```python
# 4. partial로 생성된 프롬프트에 질문만 채워 프롬프트 구성
sample_question = "현재 2025년 3월 시장 상황에서 삼성전자 주식 전망은 어떻습니까?"
messages = partial_chat_prompt.format_messages(question=sample_question)
print(messages)
```

출력되는 메시지 리스트는 다음과 같습니다:

```text
[SystemMessage(content='당신은 주식 투자 분야의 전문 지식인입니다. 가능한 한 자세히 답변해 주세요.', additional_kwargs={}, response_metadata={}),
 HumanMessage(content='현재 2025년 3월 시장 상황에서 삼성전자 주식 전망은 어떻습니까?', additional_kwargs={}, response_metadata={})]
```

보시는 것처럼 시스템 메시지의 `{role}` 부분이 `"주식 투자"`로 채워져 \*\*"당신은 주식 투자 분야의 전문 지식인입니다..."\*\*로 완료되었고, 사용자 메시지도 질문 내용으로 채워져 있습니다. 이제 이 프롬프트를 이용해 실제 GPT-4o-mini 모델의 응답을 받아보겠습니다.

## 5. 프롬프트 실행 및 체인 연결 예시 (LCEL 파이프라인)

위에서 partial을 활용해 생성한 `partial_chat_prompt`는 **주식 투자 전문가** 역할이 고정된 상태입니다. 마지막으로 이 프롬프트를 LLM에 전달하여 응답을 확인해보겠습니다. 또한, 이번 섹션에서는 앞서 코드에서 사용했던 **파이프라인(`|`) 연산자**의 의미를 간략히 설명합니다.

### LCEL 파이프라인을 통한 실행

LangChain v0.3부터 도입된 \*\*LangChain Expression Language (LCEL)\*\*는 체인 구성을 보다 간결하게 표현하기 위한 기능입니다. 파이프(`|`) 연산자는 **왼쪽의 출력**을 **오른쪽의 입력**으로 전달하는 역할을 하며, `prompt | llm` 형태로 사용하면 프롬프트 포맷팅 결과를 곧바로 LLM에 연결해 줍니다. 즉, 전통적으로 `LLMChain`을 통해 `llm(prompt.format(x))` 처리를 하던 것을 파이프로 한 줄에 표현할 수 있습니다.

> 참고: 기존 방식으로는 `chain = LLMChain(prompt=some_prompt, llm=some_llm)`를 정의한 뒤 `chain.run(input_dict)`로 실행했습니다. LCEL 파이프라인을 이용하면 이러한 과정을 `result = (some_prompt | some_llm).invoke(input_dict)`로 축약할 수 있습니다. 내부 동작은 동일하며, 표현만 함수형 파이프라인 스타일로 달라집니다.

이제 partial로 만든 프롬프트를 파이프라인을 통해 실행하고 출력 결과를 살펴보겠습니다:

```python
# partial_chat_prompt와 llm을 연결하여 답변 얻기
answer = (partial_chat_prompt | llm).invoke({"question": sample_question})
print(answer.content)
```

GPT-4o-mini 모델로부터 얻은 답변 예시는 다음과 같습니다:

```text
삼성전자 주식은 현재 시장 변동성에 영향을 받아 단기적으로 등락을 거듭할 수 있습니다. 그러나 반도체 산업의 장기적 성장성과 삼성전자의 탄탄한 재무 구조를 고려하면, 장기적인 전망은 여전히 긍정적으로 볼 수 있습니다. 투자 시 단기 변동성에 유의하면서 장기 관점에서 접근하는 것이 좋겠습니다.
```

모델은 **주식 투자 전문가**의 구체적인 관점에서 삼성전자 주식 전망에 대해 신중하고도 긍정적인 답변을 제공했습니다. 시스템 메시지에 지정한 역할에 부합하게 전문지식을 동원한 어조를 볼 수 있습니다. 이처럼 PartialPromptTemplate을 활용하면, 하나의 베이스 템플릿으로부터 다양한 고정 설정을 가진 파생 템플릿들을 만들어 낼 수 있습니다. 예를 들어 위 `base_chat_prompt`에 대해 `role="부동산 투자"`, `role="암호화폐 투자"` 등으로 partial을 수행하면 다른 분야 전문가로 답변하는 프롬프트를 바로 얻을 수 있을 것입니다.

### 결과 프롬프트 확인 (디버깅 팁)

실습 과정에서 **프롬프트 렌더링 결과**를 확인하는 것은 매우 중요합니다. 프롬프트가 의도한 대로 구성되었는지, 모든 변수가 제대로 채워졌는지 확인해야 모델이 정확한 입력을 받는다고 확신할 수 있기 때문입니다.

LangChain에서 PromptTemplate의 `format()`이나 ChatPromptTemplate의 `format_messages()`/`format_prompt()` 등을 사용하면 **완성된 프롬프트**를 출력해볼 수 있습니다. 위 예시들에서는 `print(formatted_prompt)`나 `print(messages)`를 통해 각 단계별로 생성된 프롬프트 내용을 확인했습니다. 이렇게 함으로써 프롬프트에 오탈자나 빠진 부분이 없는지, 변수가 잘 치환되었는지 검증할 수 있습니다.

또한, ChatPromptTemplate의 결과는 메시지 객체 형태이기 때문에, `formatted_chatprompt.to_messages()`나 `format_messages` 결과를 살펴봄으로써 시스템/사용자 메시지의 구조와 내용을 디버깅할 수 있습니다. 

마지막으로, 전체 프롬프트 템플릿 사용 과정을 요약하면 다음과 같습니다:

* PromptTemplate/ChatPromptTemplate으로 프로그래밍적으로 프롬프트 구성 👉
* `.format` 또는 `.format_messages` 등으로 최종 프롬프트 확인 👉
* LLMChain 또는 LCEL 파이프라인으로 LLM 실행 👉
* 모델 응답 결과 확인 및 개선 반복

이 과정을 통해 프롬프트 엔지니어링과 LLM 활용을 체계화할 수 있습니다.

## 6. 프롬프트 템플릿 작성 팁 및 주의사항

마지막으로, LangChain 프롬프트 템플릿을 작성하고 활용할 때 알아두면 좋은 실무 팁과 주의사항을 정리합니다:

* **명확성 (Clarity)**: 템플릿 문자열은 모델에게 줄 지시를 명확하고 간결하게 작성합니다. 불필요하게 복잡한 문장이나 모호한 표현을 피하고, 필요한 정보와 요청만 포함되도록 합니다. 프롬프트가 이해하기 쉬울수록 모델이 더 정확한 응답을 생성합니다.

* **일관성 (Consistency)**: 변수 이름과 말투, 형식을 일관되게 유지하세요. 예를 들어 변수 이름은 의미가 드러나게 짓고 (`product_name`, `user_input` 등), 여러 프롬프트에서 동일한 개념의 변수는 같은 이름을 사용합니다. 프롬프트 구조를 일정하게 함으로써 모델 응답의 형태도 예측 가능해지고, 유지보수 시 혼란을 줄일 수 있습니다.

* **입력 변수 검증**: 템플릿에 사용된 모든 플레이스홀더에 대해 `.format()` 호출 시 대응되는 값을 제공합니다. 값이 누락되면 오류가 발생합니다. LangChain의 PromptTemplate 생성자에는 `validate_template=True` 옵션이 있어, 템플릿 문자열과 변수 목록이 맞지 않으면 미리 경고를 주도록 할 수 있습니다. 항상 **중괄호 쌍**(`{}`)이 제대로 닫혔는지, 변수 철자가 맞는지 확인하세요.

* **부분 템플릿 활용**: PartialPromptTemplate를 적재적소에 활용하면 코드의 간결성과 재사용성이 높아집니다. 공통으로 쓰이는 프롬프트 조각(예: 시스템 역할 지시, 일정한 출력 형식 요구 등)은 partial로 미리 채워 두고, 변화하는 부분만 나중에 입력받도록 하면 편리합니다. 이는 체인을 설계할 때 앞 단계의 결과를 prompt의 일부로 넘겨줄 때도 유용합니다.

* **Few-Shot 및 예시 활용**: (본 실습에서는 다루지 않았지만) 프롬프트에 모델 답변의 예시를 포함시키고 싶다면 LangChain의 `FewShotPromptTemplate`을 사용할 수 있습니다. 이 기능을 통해 여러 개의 Q\&A 쌍이나 입출력 예시를 프롬프트에 삽입하여 모델이 답변 형식을 학습하도록 할 수 있습니다. Few-shot 예시를 추가할 때도 PromptTemplate를 사용해 변수를 채울 수 있으므로, 필요할 경우 문서를 참고하여 활용해보세요.

* **멀티라인과 포맷팅**: 긴 프롬프트는 Python의 삼중 따옴표 문자열(`""" """"`)이나 위에서처럼 여러 줄로 나누어 작성하면 가독성이 높아집니다. 단, 중괄호 `{}`는 특별한 의미를 가지므로, 만약 프롬프트 자체에 중괄호 문자를 포함해야 한다면 `{{` `}}`처럼 이스케이프해야 합니다. 또는 `PromptTemplate(template=..., template_format="jinja2")`와 같이 템플릿 포맷을 변경하여 Jinja2 스타일로 작성할 수도 있습니다. 하지만 복잡한 포맷은 초보 단계에서는 혼란을 줄 수 있으므로 가급적 기본 f-string 스타일을 사용하고, 문제가 있을 때 대안을 검토하세요.

* **API 키 및 환경관리**: 실무에서 프롬프트 실험을 할 때 OpenAI API 키를 코드에 하드코딩하지 마세요. 이번 실습처럼 `.env` 파일과 `python-dotenv`를 이용하거나, 운영 환경의 환경 변수로 주입하는 방식이 안전합니다. 키를 깃허브 등에 노출하면 안 되므로, `.env`는 `.gitignore`에 추가해 관리하고 협업 시에도 공유하지 않도록 유의합니다.

* **모델 응답 검토 및 튜닝**: 프롬프트 템플릿을 설계한 뒤에는, 다양한 입력에 대해 모델 응답을 관찰하며 프롬프트 문구를 개선하는 과정이 필요합니다. 원하는 출력 형식이나 스타일이 나오지 않으면 지시를 더 추가하거나, 불필요한 부분은 줄여가면서 **실험적으로 튜닝**합니다. 예를 들어 "\~\~형식으로 답변해줘"나 "목록으로 출력해줘" 등의 지시를 템플릿에 넣어 원하는 결과를 유도할 수 있습니다. 다만 지나치게 세세한 지시는 모델이 따라주지 못하거나 혼란스러워할 수 있으므로, **핵심만 명시**하는 것이 좋습니다.

* **비용과 성능 고려**: 프롬프트 길이가 길어질수록 API 호출 비용과 응답 지연이 증가합니다. 필요한 내용 위주로 간결하게 작성하고, 중복되는 부분은 partial 등으로 효율적으로 관리하세요. 또한 temperature, max\_tokens 등의 매개변수를 LLM 초기화 시 조정하여 출력의 창의성이나 분량을 통제할 수 있으니 활용하시기 바랍니다.

위 내용을 참고하여 프롬프트 템플릿을 설계하면, 보다 **효과적이고 관리하기 쉬운 LLM 활용**이 가능해집니다. LangChain의 프롬프트 템플릿은 **재사용성과 구성 가능성**을 높여주므로, 다양한 응용 시나리오에서 유용하게 활용하시길 바랍니다.

---

이상으로 **LangChain PromptTemplate과 ChatPromptTemplate**의 핵심 기능과 활용법을 실습해보았습니다. 프롬프트 템플릿은 LLM 개발에 있어 반복적인 프롬프트 작성 작업을 효율화하고, 일관된 성능을 이끌어내는 강력한 도구입니다. 앞으로 실제 프로젝트에서 필요에 따라 PromptTemplate을 구성하고, partial이나 chat 템플릿 등의 기법을 응용해보세요. 꾸준한 프롬프트 실험과 개선을 통해 더 나은 AI 응답을 얻을 수 있을 것입니다.
