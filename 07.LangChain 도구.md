# LangChain Tools

LangChain 프레임워크와 OpenAI의 GPT-4o mini 모델을 활용하여 **LLM (Large Language Model)이 외부 기능(툴)을 호출**하는 방법을 알아보겠습니다. 이 가이드는 파이썬 중급 개발자를 대상으로 하며, Jupyter 노트북이나 VS Code 환경에서 실습할 수 있도록 단계별 코드 예제와 설명을 제공합니다. 또한 API 키 등 **환경변수는 `.env` 파일에 저장**하고 `python-dotenv`로 불러오는 방식을 사용합니다.

## 1. LangChain Tool의 개념과 구조

* **Tool이란?** LangChain에서 \*툴(Tool)\*은 LLM 에이전트가 사용할 수 있는 **외부 기능**이나 **API**를 말합니다. 예를 들어 날씨 정보 조회, 웹 검색, 계산기 등 LLM이 자체 지식만으로 해결할 수 없는 작업을 수행하기 위한 도구입니다.
* **왜 필요한가?** GPT 같은 LLM은 훈련 데이터 이후의 **최신 정보에 접근할 수 없고**, 수학 계산이나 데이터베이스 질의 등 **정확한 처리**가 필요한 작업에 한계가 있습니다. 툴을 사용하면 LLM이 이러한 한계를 극복하여 **최신 정보 조회**나 **정확한 연산** 등을 수행할 수 있습니다.
* **구조:** LangChain에서 Tool은 보통 **이름(name)**, **설명(description)**, 그리고 **함수 실행 로직**으로 구성됩니다. 이름과 설명은 LLM이 어떤 상황에 어떤 툴을 써야 할지 판단하는 데 사용됩니다. 함수는 실제로 툴이 수행하는 Python 코드이며, 입력을 받아 외부 API 호출이나 계산 등을 수행한 후 **문자열 결과**를 반환합니다.
* **툴 호출 방식:** 최신 GPT-4 모델들은 **OpenAI의 함수 호출(Function Calling)** 기능을 통해 툴을 직접 호출할 수 있습니다. LangChain은 이러한 기능을 활용하여, LLM이 적절한 함수(툴)를 **함수명과 인자** 형태로 호출하면 개발자가 해당 함수를 실행하고 결과를 다시 LLM에게 전달하는 **에이전트 루프**를 구현합니다. LLM은 여러 툴을 연속으로 사용한 뒤 최종 **자연어 답변**을 생성합니다.

## 2. 외부 API 호출 Tool 구현하기 (예: 날씨 API, 뉴스 API)

이 절에서는 실제 외부 API를 호출하는 커스텀 툴을 구현해보겠습니다. 예시로 **날씨 정보 조회 툴**과 **뉴스 검색 툴** 두 가지를 만들어보겠습니다.

### 2.1 환경 설정 및 라이브러리 설치

우선 필요 라이브러리를 설치하고 API 키를 설정합니다.

* **필요 패키지:** LangChain (코어 및 OpenAI 모듈), OpenAI Python SDK, `python-dotenv`(환경변수 관리), `requests`(API 호출) 등이 필요합니다. 아래와 같이 설치합니다.

```bash
pip install langchain-core langchain-openai openai python-dotenv requests
```

* **API 키 준비:** OpenAI API 키와 OpenWeatherMap, NewsAPI 등의 키를 발급받아 `.env` 파일에 저장합니다. (.env 파일은 노트북 디렉터리에 두고 다음과 같은 형식으로 작성합니다.)

```
OPENAI_API_KEY=<발급받은 OpenAI 키>
OPENWEATHERMAP_API_KEY=<OpenWeatherMap 키>
NEWSAPI_API_KEY=<NewsAPI 키>
```

이제 파이썬에서 dotenv를 통해 환경변수를 불러오겠습니다.

```python
# .env 파일 로드
from dotenv import load_dotenv
import os

load_dotenv()  # .env 파일의 환경변수 로드
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OWM_API_KEY = os.getenv("OPENWEATHERMAP_API_KEY")
NEWS_API_KEY = os.getenv("NEWSAPI_API_KEY")

# 키 값 확인 (None이 아니라면 성공적으로 불러온 것입니다)
print("OpenAI Key present?", OPENAI_API_KEY is not None)
print("Weather API Key present?", OWM_API_KEY is not None)
print("News API Key present?", NEWS_API_KEY is not None)
```

### 2.2 날씨 정보 조회 툴 구현

OpenWeatherMap의 **Current Weather Data API**를 호출하여 특정 도시의 날씨를 가져오는 함수를 작성해보겠습니다. 이 함수는 도시 이름을 입력받아 현재 기온과 날씨 상태를 문자열로 리턴합니다.

```python
import requests

# 입력 스키마 정의
class WeatherInput(BaseModel):
    city: str = Field(description="날씨를 조회할 도시 이름 (영문)")

def get_weather(city: str) -> str:
    """주어진 도시의 현재 날씨를 반환합니다."""
    api_key = os.getenv("OPENWEATHERMAP_API_KEY")
    url = "http://api.openweathermap.org/data/2.5/weather"
    params = {
        "q": city,
        "appid": api_key,
        "units": "metric",      # 섭씨 온도
        "lang": "kr"            # 한국어 응답 (설명이 한국어로 오도록)
    }
    response = requests.get(url, params=params)
    data = response.json()
    # API 응답에 따른 처리
    if data.get("cod") != 200:
        # 도시 정보를 찾지 못한 경우
        return f"'{city}'의 날씨 정보를 찾을 수 없습니다."
    # 필요한 정보 파싱
    temp = data["main"]["temp"]
    desc = data["weather"][0]["description"]
    city_name = data["name"]  # 응답에 있는 도시 이름 (영문일 수 있음)
    return f"{city_name}의 현재 기온은 {temp}℃, 날씨는 {desc}입니다."
```

위 코드에서 `units=metric`으로 설정하여 온도를 섭씨로 받고, `lang=kr`을 사용해 날씨 설명(`description`)을 한국어로 요청했습니다. `get_weather("Seoul")`처럼 호출하면 서울의 현재 날씨 정보를 얻을 수 있습니다.

**실행 예시:**

```python
print(get_weather("Seoul"))
```

예를 들어 서울(Seoul)의 날씨가 맑고 18℃인 경우, 출력 결과는 다음과 비슷하게 나올 것입니다.

```
Seoul의 현재 기온은 18℃, 날씨는 맑음입니다.
```

> **Note:** 사용자가 도시명을 한글로 입력할 수 있습니다. OpenWeatherMap API는 도시명을 영문으로 요청하는 것이 일반적이므로, 사용자가 "서울"처럼 입력한 경우 적절히 영문 ("Seoul")으로 변환하거나, `q` 파라미터에 `"Seoul,KR"` 등 국가 코드를 포함하는 방식을 고려해야 합니다. (이번 예시에서는 간단히 영문 도시명을 사용하는 것으로 가정합니다.)

### 2.3 뉴스 검색 툴 구현

두 번째로 **뉴스 검색 툴**을 구현해보겠습니다. NewsAPI를 사용하여 특정 키워드에 대한 최신 뉴스를 가져오는 함수를 작성합니다. 뉴스 헤드라인과 간략한 설명을 반환하도록 합시다.

```python
# 입력 스키마 정의
class NewsInput(BaseModel):
    keyword: str = Field(description="최신 뉴스를 검색할 키워드 (한글 또는 영문)")

def get_news(keyword: str) -> str:
    """주어진 키워드에 대한 최신 뉴스 제목을 반환합니다."""
    api_key = os.getenv("NEWSAPI_API_KEY")
    url = "https://newsapi.org/v2/everything"
    params = {
        "q": keyword,
        "language": "ko",         # 한국어 뉴스 기사 검색
        "sortBy": "publishedAt",  # 최신 뉴스 우선 정렬
        "pageSize": 1,            # 가장 최근 기사 1건만 가져오기
        "apiKey": api_key
    }
    res = requests.get(url, params=params)
    data = res.json()
    articles = data.get("articles")
    if not articles:
        return f"'{keyword}'에 대한 최신 뉴스가 없습니다."
    # 가장 첫 번째 뉴스를 선택
    top_article = articles[0]
    title = top_article.get("title", "(제목 없음)")
    source = top_article.get("source", {}).get("name", "")
    return f"'{keyword}' 관련 뉴스: {title} - {source}"
```

이 함수는 주어진 키워드로 뉴스 기사를 검색하고, 가장 최신 기사 하나의 제목과 출처를 문자열로 반환합니다.

**실행 예시:**

```python
print(get_news("OpenAI"))
```

예를 들어 "OpenAI" 키워드에 대한 뉴스가 있다면 다음과 같은 형식으로 결과가 나올 것입니다.

```
'OpenAI' 관련 뉴스: OpenAI, 조니 아이브의 AI 스타트업 'io' 65억 달러에 인수 - 연합뉴스
```

위 예시에서는 OpenAI와 관련된 최신 뉴스 제목과 출처(연합뉴스)를 보여주고 있습니다. (실제 결과는 시점에 따라 달라질 수 있습니다.)

### 2.4 LangChain Tool 객체로 변환

이제 작성한 `get_weather`와 `get_news` 함수를 LangChain의 Tool로 등록하겠습니다. LangChain v0.3.x에서는 **Runnable** 개념을 활용해 함수형 툴을 만들 수 있습니다. `RunnableLambda`를 사용하여 파이썬 함수를 감싸고, `.as_tool()` 메서드로 Tool 객체로 변환하겠습니다.

```python
from langchain_core.runnables import RunnableLambda

# 날씨 함수 runnable -> tool 변환
weather_runnable = RunnableLambda(lambda input: get_weather(input["city"]))
weather_tool = weather_runnable.as_tool(
    name="get_weather",
    description="도시 이름(영문)을 입력하면 현재 날씨를 알려주는 도구",
    args_schema=WeatherInput
)

# 뉴스 함수 runnable -> tool 변환
news_runnable = RunnableLambda(lambda input: get_news(input["keyword"]))
news_tool = news_runnable.as_tool(
    name="get_news",
    description="키워드를 입력하면 관련된 최신 뉴스 제목을 알려주는 도구",
    args_schema=NewsInput
)

print(f"Tool 이름: {weather_tool.name}, 설명: {weather_tool.description}")
print(f"Tool 이름: {news_tool.name}, 설명: {news_tool.description}")
```

위 코드에서는 `get_weather`와 `get_news`를 각각 `RunnableLambda`로 감싼 뒤 `as_tool`로 LangChain **툴 객체**를 생성했습니다. `name`과 `description`을 지정했는데, 이 정보는 LLM이 툴을 선택하고 사용하는 데 매우 중요합니다.

출력으로 각 툴의 이름과 설명을 확인할 수 있습니다:

```
Tool 이름: get_weather, 설명: 도시 이름(영문)을 입력하면 현재 날씨를 알려주는 도구  
Tool 이름: get_news, 설명: 키워드를 입력하면 관련된 최신 뉴스 제목을 알려주는 도구
```

이제 우리가 정의한 두 가지 툴이 LangChain에서 사용 가능한 형태로 준비되었습니다.

> **참고:** `as_tool` 메서드를 사용하면 입력 인자에 대한 스키마도 자동 설정됩니다. 우리는 문자열 입력을 받는 간단한 형태이므로 기본 설정을 사용했습니다. 복잡한 입력이 필요한 경우 `args_schema` 등을 정의하여 구조화된 툴(Structured Tool)을 만들 수 있습니다.

## 3. 단일 Tool을 직접 호출하는 방식 실습

이 절에서는 툴을 **에이전트를 쓰지 않고 직접 호출**하여 LLM의 답변에 활용하는 방법을 연습해보겠습니다. 시나리오는 간단합니다. \*"사용자가 어떤 질문을 했고, 우리가 적절한 툴을 호출하여 얻은 정보"\*를 LLM에게 전달해서 최종 답변을 생성하도록 합니다.

예를 들어, 사용자가 **"서울 날씨 알려줘."** 라고 질문했다고 가정해보겠습니다. 이 질문에는 최신 날씨 정보가 필요하므로 저희는 미리 정의한 `get_weather` 툴을 사용할 수 있습니다.

### 3.1 툴 직접 호출하여 정보 얻기

우선 사용자의 질문을 분석하여 필요한 툴과 인자를 결정합니다. 여기서는 \*"서울 날씨"\*라는 키워드를 보고 날씨 툴이 필요함을 판단했다고 가정합니다. 한글 "서울"을 OpenWeatherMap에서 인식하도록 영문 "Seoul"로 변환해 호출하겠습니다.

```python
user_question = "서울 날씨 알려줘."
# 1단계: Tool 함수 직접 호출
city_query = "Seoul"  # "서울"을 API에 맞게 영문 변환
weather_info = get_weather(city_query)
print("[툴 출력]", weather_info)
```

```
[툴 출력] Seoul의 현재 기온은 18℃, 날씨는 맑음입니다.
```

위 결과로부터, 서울의 현재 날씨 정보를 얻었습니다. (여기서는 예시로 임의의 온도와 날씨 상태를 출력했습니다.)

### 3.2 LLM에게 최종 답변 생성 요청

다음으로, LLM에게 이 정보를 전달하여 사용자 질문에 대한 **자연스러운 최종 답변**을 작성하게 합니다. OpenAI의 GPT-4o mini 모델을 LangChain을 통해 불러와보겠습니다. 환경변수에 저장한 API 키와 모델 이름을 사용합니다. (GPT-4o mini 모델명을 사용하지만, API 사용법은 GPT-3.5나 GPT-4와 동일합니다.)

```python
from langchain.chat_models import ChatOpenAI
from langchain.schema import AIMessage, HumanMessage, SystemMessage

# OpenAI GPT-4o mini 모델 초기화 (온도 0으로 설정해 일관된 답변 유도)
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0,
    openai_api_key=os.getenv("OPENAI_API_KEY")
)

# 프롬프트 구성: 시스템 메시지 + 사용자 질문 + 툴 결과를 맥락으로 제공
system_prompt = "당신은 유용한 AI 어시스턴트입니다. 사용자 질문에 맞게 제공된 정보들을 활용해 답변하세요."
user_prompt = user_question  # "서울 날씨 알려줘."
tool_info_prompt = f"도구가 제공한 추가 정보: {weather_info}"
ask_prompt = "위 정보를 참고하여 사용자 질문에 답변해주세요."

messages = [
    SystemMessage(content=system_prompt),
    HumanMessage(content=user_prompt),
    AIMessage(content=tool_info_prompt),   # 도구 결과를 마치 AI가 말한 내용처럼 추가
    HumanMessage(content=ask_prompt)
]

result = llm(messages)
print(result.content)
```

위에서 프롬프트를 구성한 방법을 살펴보면:

* **SystemMessage:** AI에게 역할 지시를 합니다. ("유용한 어시스턴트"이며 "제공된 정보를 활용해 답변"하도록 안내)
* **HumanMessage:** 실제 사용자 질문 ("서울 날씨 알려줘.")
* **AIMessage:** 도구가 반환한 정보를 대화 맥락에 추가했습니다. 마치 AI가 미리 정보를 말해준 것처럼 구성하여, LLM이 이 정보를 참고할 수 있게 합니다.
* **HumanMessage:** 마지막으로 사용자에게 최종 답변을 요청하는 형태로 프롬프트를 마무리합니다.

프롬프트를 구성하는 다른 방법도 있지만, 여기서는 간단히 **대화 맥락**에 정보와 지시를 넣는 형태를 사용했습니다. 이제 GPT-4o mini 모델이 이 메시지들을 읽고 최종 답변을 생성하게 됩니다.

**예상 답변 출력:**

```
현재 서울은 기온 18℃에 맑은 하늘을 보이고 있어요. 쾌청한 날씨를 즐기시길 바랍니다!
```

LLM이 툴에서 얻은 정보를 토대로 한국어로 자연스러운 문장을 생성한 것을 볼 수 있습니다. 이러한 방식으로, 하나의 툴 결과를 우리가 직접 LLM에 제공하여 답변을 만들 수 있습니다.

> **Note:** 이 접근법은 **에이전트를 사용하지 않고** 프로그래머가 툴 호출과 LLM 응답 생성을 수동으로 조율하는 방식입니다. 장점은 흐름이 명시적이라 디버깅이 쉽다는 점이고, 단점은 툴의 종류가 많아질수록 일일이 분기 처리가 필요해진다는 것입니다. 다음 절에서는 **여러 툴을 LLM 에이전트가 스스로 선택하도록** 자동화하는 방법을 알아보겠습니다.

## 4. 여러 Tool을 사용하도록 에이전트 구성하기

이제 두 개의 툴 (`get_weather`, `get_news`)을 **에이전트**에 등록하고, **LLM이 스스로 어떤 툴을 쓸지 결정**하여 문제를 해결하는 흐름을 실습해보겠습니다. LangChain에서는 OpenAI의 함수 호출 기능을 활용한 에이전트를 지원하므로, GPT-4o mini 모델이 **함수 형태로 툴을 호출**하고 그 결과를 받아 최종 답을 만들어내게 할 수 있습니다.

### 4.1 에이전트 생성 및 툴 등록

LangChain의 `initialize_agent` 함수를 사용하여 에이전트를 생성하겠습니다. OpenAI의 함수 호출 기반 에이전트를 쓰기 위해 `agent=AgentType.OPENAI_FUNCTIONS` 옵션을 지정합니다. 이 에이전트는 툴의 name/description을 참고하여 GPT 모델이 `function_call` 형태로 적절한 함수를 호출하게 합니다.

```python
from langchain.agents import initialize_agent, AgentType

# 앞서 만든 Tool 객체 리스트 준비
tools = [weather_tool, news_tool]

# OpenAI 함수 기반 에이전트 생성
agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.OPENAI_FUNCTIONS,
    verbose=True  # intermediate 단계 로그를 출력하여 확인
)
```

위에서 `tools` 리스트로 두 개의 툴을 전달했고, LLM으로는 GPT-4o mini(`llm`)를 사용했습니다. `verbose=True`로 설정하면 에이전트가 툴을 선택/호출하는 과정을 로그로 볼 수 있어 이해에 도움이 됩니다.

### 4.2 에이전트 실행 예시 1: 단일 툴 선택

첫 번째 예시로 **날씨 정보만 필요한 질문**을 던져보겠습니다. 예를 들어: **"뉴욕의 날씨가 어떤지 알려줘."** 라는 요청을 에이전트에 주겠습니다. (여기서는 "뉴욕"을 영문 "New York"으로 인식할 수 있도록 질문에 병기했습니다.)

```python
query1 = "뉴욕(New York)의 날씨가 어떤지 알려줘."
result1 = agent.run(query1)
print("Agent Result 1:", result1)
```

`verbose=True`이므로 내부 과정이 출력됩니다. GPT-4o mini 모델은 질문을 읽고, "날씨" 관련 요청이므로 `get_weather` 툴을 호출하려 할 것입니다. 로그의 **Thought/Action** 부분에서 이러한 판단을 확인할 수 있습니다:

```
> Entering new AgentExecutor chain...
Thought: 사용자의 질문은 뉴욕의 현재 날씨를 묻고 있다. 날씨 정보를 얻기 위해 get_weather 툴을 사용해야겠다.
Action: get_weather
Action Input: "New York"
Observation: New York의 현재 기온은 25℃, 날씨는 구름 조금입니다.
Thought: 날씨 정보를 얻었으니, 이를 바탕으로 사용자에게 답변을 작성해야겠다.
Final Answer: 현재 뉴욕은 기온 25℃에 구름이 조금 낀 날씨입니다.
> Finished chain.
```

위 로그를 보면 에이전트(LangChain+LLM)가 스스로 **Thought**를 통해 어떤 툴을 쓸지 결정하고(**Action**), `get_weather`를 호출한 뒤 **Observation**(툴 출력)을 받아 최종 답변(**Final Answer**)을 만들었습니다. 최종 답변은 `result1` 변수에 문자열로 반환되며, 출력 예시는 다음과 같습니다.

```
Agent Result 1: 현재 뉴욕은 기온 25℃에 구름이 조금 낀 날씨입니다.
```

에이전트가 `get_weather` 툴을 정확히 선택하여 결과를 활용한 것을 볼 수 있습니다. 이처럼 사용자의 질문 의도에 따라 여러 툴 중 하나를 **자동으로 선택**할 수 있습니다.

### 4.3 에이전트 실행 예시 2: 복수 툴 활용

두 번째 예시로 **두 가지 툴을 모두 사용하는 질문**을 시도해보겠습니다. 예를 들어 **"서울의 날씨와 OpenAI에 대한 최신 뉴스 알려줘."** 같은 복합 질문을 해보겠습니다. 이 경우 에이전트는 날씨도 조회하고 뉴스도 찾아야 합니다.

```python
query2 = "서울의 날씨와 OpenAI에 대한 최신 뉴스 알려줘."
result2 = agent.run(query2)
print("Agent Result 2:", result2)
```

에이전트의 내부 추론 로그를 살펴보면 다음과 같이 **두 단계의 툴 호출**이 이루어집니다:

```
Thought: 사용자 요청은 1) 서울 날씨, 2) OpenAI 최신 뉴스 두 정보를 모두 요구한다.
Action: get_weather 
Action Input: "Seoul"
Observation: Seoul의 현재 기온은 18℃, 날씨는 맑음입니다.
Thought: 서울 날씨 정보를 얻었다. 이제 OpenAI 관련 최신 뉴스를 얻어야 한다.
Action: get_news 
Action Input: "OpenAI"
Observation: 'OpenAI' 관련 뉴스: OpenAI, 조니 아이브의 AI 스타트업 'io' 65억 달러에 인수 - 연합뉴스
Thought: 두 가지 정보를 모두 얻었으니, 이를 조합해 답변을 작성하자.
Final Answer: 현재 서울의 기온은 18℃로 맑은 날씨입니다. 또한, 최신 뉴스에 따르면 OpenAI가 조니 아이브의 AI 스타트업 'io'를 약 65억 달러에 인수했다고 합니다.
```

위 과정에서 GPT-4o mini는 먼저 `get_weather`를 호출해 서울의 날씨를 얻고, 이어서 `get_news`를 호출해 OpenAI 관련 뉴스를 얻었습니다. 그리고 마지막으로 두 결과를 합쳐서 하나의 답변을 구성했습니다. `result2`에 담긴 최종 답변을 출력해보면:

```
Agent Result 2: 현재 서울의 기온은 18℃로 맑은 날씨입니다. 또한, 최신 뉴스에 따르면 OpenAI가 조니 아이브의 AI 스타트업 'io'를 약 65억 달러에 인수했다고 합니다.
```

이처럼 에이전트를 사용하면 **LLM 자체가 어떤 툴을 어느 순서로 호출할지 결정**하여, 필요한 정보를 수집하고 종합하는 과정을 자동화할 수 있습니다. 개발자는 툴만 제공하고, **질문에 따른 툴 선택 로직은 LLM에 맡기는 것**입니다.

> **Note:** `verbose=True` 설정으로 인해 Thought/Action 로그를 확인할 수 있었는데, 실제 애플리케이션에서는 이러한 내부 추론이 사용자에게 노출되지 않도록 해야 합니다. 기본적으로 LangChain 에이전트는 최종 답변만 사용자에게 출력하고, 중간 과정은 숨깁니다.

## 5. 툴 결과를 자연어 답변으로 통합하기 위한 프롬프트 작성

툴을 사용한 뒤 **LLM이 최종 답변을 잘 구성하도록 유도하는 프롬프트 작성**도 중요합니다. LLM이 툴 호출 결과만 나열하지 않고, 사용자에게 **매끄러운 완성형 답변**을 제공해야 하기 때문입니다. 이 절에서는 그런 프롬프트 작성 팁을 정리해보겠습니다.

* **시스템 역할 지시**: LLM이 툴을 사용한 뒤 \*“최종적으로 사용자에게 친절하고 자연스러운 답변을 제공해야 한다”\*는 점을 시스템 메시지 등으로 명확히 지시하는 것이 좋습니다. 예를 들어, 시스템 프롬프트에 \*"도구 사용 과정을 노출하지 말고, 얻은 정보를 토대로 완결된 답변만 하라"\*는 내용을 넣을 수 있습니다. 이러한 지시는 에이전트 프롬프트 템플릿에 기본적으로 포함되어 있습니다.

* **언어 및 구체적 요청**: 사용자 질문의 언어에 맞춰 답변하도록 유도하세요. 한국어 질문이면 답변도 한국어로, 그리고 가능하면 **존댓말** 등 적절한 어조를 사용하도록 합니다. 예를 들어, *"사용자의 질문에 한국어로 정중하게 답변하세요."* 같은 지시를 추가할 수 있습니다. GPT-4o mini는 다국어를 지원하므로, 이런 지시가 없더라도 보통 사용자 메시지의 언어로 답변하지만, 명시하면 더 확실합니다.

* **툴 결과 활용**: 툴에서 얻은 데이터를 답변에 **재가공**하거나 **설명**을 덧붙일 수 있습니다. 예를 들어 날씨 정보라면 숫자나 상태만 나열하기보다, \*"쾌청한 날씨이니 나들이하기 좋겠습니다."\*처럼 한 문장을 더 붙이면 답변이 풍부해집니다. 이런 부분은 프롬프트에서 직접 지시하기보다는, 시스템 역할에서 어시스턴트의 성격을 친절하게 설정하거나, 몇 번의 예시를 통해 학습시킬 수 있습니다.

* **프롬프트 템플릿 활용**: LangChain에서는 `ChatPromptTemplate` 등을 사용해 메시지 템플릿을 구조화할 수 있습니다. 예를 들어, 아래처럼 템플릿을 만들어두면 매번 일관된 지시를 내리기 쉽습니다.

  ```python
  from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate

  system_template = "당신은 유능한 비서입니다. 주어진 정보를 활용해 완결된 답변만 제공합니다.\n"
  human_template = "{question}\n"
  info_template = "추가 정보: {tool_output}\n"
  request_template = "위 정보를 참고하여 질문에 답해주세요."

  prompt = ChatPromptTemplate.from_messages([
      SystemMessage(content=system_template),
      HumanMessagePromptTemplate.from_template(human_template),
      HumanMessagePromptTemplate.from_template(info_template),
      HumanMessagePromptTemplate.from_template(request_template),
  ])

  # 예시 사용
  final_messages = prompt.format_messages(question="서울 날씨 알려줘.", tool_output="Seoul의 현재 기온은 18℃, 날씨는 맑음입니다.")
  for m in final_messages:
      print(m.type, ":", m.content)
  ```

  위와 같이 하면 프롬프트 작성이 체계화되고, 여러 상황에서도 공통된 형식을 유지할 수 있습니다. (에이전트를 직접 구성할 때는 `create_openai_functions_agent`를 사용할 수도 있지만, 여기서는 개념 설명을 위해 수동 템플릿 예시를 들었습니다.)

* **예상 답변 검토**: 툴의 결과를 받은 LLM의 답변이 만족스러운지 항상 검토해야 합니다. 잘못된 정보가 들어가거나 맥락에 안 맞는 경우, 프롬프트를 수정하거나 툴의 출력 포맷을 조정하는 후처리가 필요할 수 있습니다. 예를 들어 툴 결과가 너무 복잡한 형식(표/json 등)이면, LLM이 이해하기 어렵지 않도록 **후처리하여 간결한 문자열로** 넘겨주는 것이 좋습니다.

以上的提示确保LLM能够根据工具提供的信息生成连贯且自然的答案。특히 에이전트 사용 시에는 대부분 이 과정이 자동으로 이뤄지지만, **시스템 프롬프트 설계**를 통해 답변 어조나 형식을 제어할 수 있다는 점을 기억하세요.

> **참고:** LangChain의 OpenAI 함수 기반 에이전트 (`AgentType.OPENAI_FUNCTIONS`)는 내부적으로 **프롬프트에 툴 설명을 포함**하고, 모델이 `function_call`을 통해 결과를 얻은 뒤 **자동으로 최종 답변만 반환**하도록 설계되어 있습니다. 따라서 기본 설정만으로도 LLM이 툴 사용 과정을 드러내지 않고 결과를 종합해 답변합니다. 필요하다면 `agent_kwargs` 등을 사용해 커스텀 시스템 메시지를 전달할 수도 있습니다.

## 마무리 및 추가 참고

이로써 LangChain v0.3.x에서 **툴을 활용하는 방법**을 단계별로 실습해보았습니다. 요약하면 다음과 같습니다:

* 외부 API를 호출하는 파이썬 함수를 작성하고, LangChain의 `RunnableLambda.as_tool`을 통해 **툴로 등록**한다.
* 단순한 경우 툴을 개발자가 직접 호출하고 그 결과를 LLM 프롬프트에 포함시켜 **답변 생성**에 활용할 수 있다.
* 복잡한 경우 LangChain **에이전트**에 여러 툴을 등록하고, **LLM의 판단**으로 필요한 툴을 순차적으로 호출하여 문제를 해결하도록 할 수 있다.
* LLM이 툴 결과를 활용해 **자연스러운 최종 답변**을 내도록 프롬프트를 세심하게 설계해야 한다.

마지막으로, 툴의 결과를 후처리하거나 품질을 평가하는 과정도 고려할 수 있습니다. 예를 들어 툴 출력에 불필요한 세부정보가 있다면 LLM에 전달하기 전에 가공하거나, 최종 답변의 정확성을 검증하는 단계가 필요할 수 있습니다. 이런 부분은 응용에 따라 달라지므로, 이번 수업에서는 간단히 언급만 하겠습니다.

이번 가이드의 내용을 바탕으로, 여러분은 자신의 LLM 에이전트에 다양한 툴을 접목시켜 볼 수 있습니다. 날씨와 뉴스 외에도 계산기, 데이터베이스 질의, 파일 읽기 등 필요한 기능을 툴로 구현해보세요. **LangChain 툴과 GPT-4o mini의 결합**을 통해 더욱 강력하고 똑똑한 AI 어시스턴트를 만들 수 있을 것입니다.
