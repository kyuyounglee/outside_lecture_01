# LangChain 출력 파서(OutputParser)

LangChain에서 **출력 파서(OutputParser)**는 LLM의 출력을 원하는 형태로 구조화해주는 핵심 컴포넌트입니다. 본 자료에서는 LangChain v0.3.x와 OpenAI의 GPT-4o-mini 모델을 활용하여 다양한 출력 파서를 실습합니다. 모든 프롬프트와 코드는 **Python**으로 작성되며, Jupyter 노트북 또는 VS Code 환경을 전제로 합니다. API 키는 `python-dotenv`를 이용해 `.env` 파일에 저장하고 불러오는 방식을 사용합니다.

짧은 Q\&A 답변부터 JSON 형식 데이터까지 출력 파서를 통해 **응답을 정형화**함으로써, 후속 처리나 평가, 다른 시스템과의 연동을 쉽게 할 수 있습니다. 

## 🎯 학습 목표

* **StrOutputParser**, **JsonOutputParser**, **CommaSeparatedListOutputParser**의 동작 방식과 차이점을 이해한다.
* 각 출력 파서를 실제 **프롬프트와 LLM 응답**에 적용하고, **파싱된 결과를 후속 처리**하는 방법을 실습한다.
* 출력 파서를 사용하는 목적과 장점: 응답의 **구조화된 후속 처리**, **평가/검증**, **외부 시스템 연동** 등을 위한 응답 정형화의 중요성을 파악한다.
* 다양한 **출력 형식** 예시를 다룬다: JSON 객체, **단일 정답 추출**, **불릿 포인트 요약 리스트** 등.

## 1. 환경 설정 (설치 및 API 키 구성)

우선 실습에 필요한 환경을 구성합니다. LangChain과 OpenAI API, dotenv 라이브러리를 설치하고 API 키를 설정합니다.

1. **필수 패키지 설치:** 터미널이나 Jupyter 노트북에서 다음 명령으로 LangChain, OpenAI, python-dotenv를 설치합니다. (이미 설치되어 있다면 생략 가능)

   ```bash
   pip install langchain openai python-dotenv
   ```
2. **OpenAI API 키 준비:** OpenAI 계정에서 발급받은 API 키를 사용합니다. 프로젝트 루트 경로에 `.env` 파일을 생성하고 아래와 같이 키를 저장합니다. (파일이 이미 존재한다면 해당 부분에 추가)

   ```
   OPENAI_API_KEY=sk-***********************
   ```
3. **환경 변수 로드:** Python 코드에서 `dotenv`를 사용하여 `.env` 파일의 API 키를 불러옵니다. 그리고 LangChain의 `ChatOpenAI` 객체를 초기화합니다. 모델은 GPT-4o-mini를 사용하고, 응답 포맷 일관성을 위해 `temperature=0`으로 설정합니다.

   ```python
   # .env 파일 로드 및 OpenAI API 키 설정
   from dotenv import load_dotenv
   import os

   load_dotenv()  # .env 파일의 환경변수 불러오기
   openai_api_key = os.environ.get("OPENAI_API_KEY")
   if openai_api_key is None:
       raise ValueError(".env 파일에 OPENAI_API_KEY를 설정해주세요!")

   # LangChain 및 OpenAI LLM 초기화
   from langchain_openai import ChatOpenAI
   llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)
   print("LLM Ready:", llm)
   ```

   ```python
   # 출력:
   # LLM Ready: ChatOpenAI(...)
   ```

   위 코드로 OpenAI API 키가 로드되고 GPT-4o-mini 모델에 연결된 `llm` 객체가 준비되었습니다. 이제 이 LLM을 사용해 LangChain 체인을 구성하고, 출력 파서를 적용할 수 있습니다.

## 2. LangChain 체인과 .invoke() 메서드

LangChain v0.3.x에서는 체인의 구성 요소들을 **파이프라인**처럼 결합하고 `.invoke()` 메서드로 실행할 수 있습니다. 체인을 구성하는 기본 요소는 **프롬프트 템플릿** (PromptTemplate), **LLM** (예: ChatOpenAI), 그리고 **OutputParser**입니다. 이들을 `|` 연산자로 연결하면 하나의 거대한 체인(Runnable 체인)이 만들어집니다.

* **PromptTemplate**: 프롬프트 문자열과 변수 플레이스홀더를 정의합니다. 예를 들어 `template="질문: {question}\n답변:"`은 `{question}` 부분에 실제 질문이 삽입되는 프롬프트 템플릿입니다.
* **LLM 모델**: OpenAI GPT-4o-mini와 같은 언어 모델로, PromptTemplate의 결과를 입력받아 답변을 생성합니다.
* **OutputParser**: LLM의 출력(텍스트)을 지정한 구조로 **파싱**합니다. 파서에 따라 문자열, 리스트, JSON 등으로 변환되어 Python 객체로 리턴됩니다.

이렇게 구성된 체인은 `.invoke(input)` 메서드로 실행합니다. 예를 들어 `chain.invoke({"question": "서울은 어느 나라의 수도인가?"})`와 같이 입력을 전달하면 체인이 동작하여 최종 파싱된 결과를 반환합니다. `.invoke()`는 LangChain의 새로운 체인 실행 방식으로, 과거 `chain.run()`과 유사하지만 보다 일관된 인터페이스를 제공합니다.

또한 체인에 `.with_config({"verbose": True})`를 적용하면 실행 시 상세한 내부 로그를 출력하도록 **verbose 모드**를 활성화할 수 있습니다. 디버깅 시 유용하며, 이에 대해서는 뒤에 자세히 다룹니다.

다음 섹션부터는 각 OutputParser의 원리와 사용법을 이 체인 구조에 맞추어 설명합니다.

## 3. StrOutputParser – 문자열 출력 파서

**StrOutputParser**는 가장 기본적인 출력 파서로, LLM의 출력 텍스트를 **있는 그대로 문자열로 반환**합니다. 특별한 변환이나 구조화 없이, \*\*가장 가능성 높은 1개의 응답(Generation)\*\*의 내용을 추출하는 역할을 합니다. 사실상 출력에 손을 대지 않고 그대로 전달하기 때문에, 체인 사용 시 기본 동작과 크게 다르지 않지만, LangChain 체인의 일환으로 다른 파서들과 동일하게 활용할 수 있다는 장점이 있습니다.

**주요 특징:**

* 입력 프롬프트에 대한 LLM의 답변을 수정 없이 문자열로 반환합니다.
* `get_format_instructions()` 메서드를 제공하지만, 별도의 포맷 지침이 필요하지 않으므로 구현이 되어 있지 않을 수 있습니다 (사용 시 NotImplementedError 발생 가능).
* 주로 응답 전체를 그대로 전달하거나, 한 단어/한 문장 등 **단일 정답 추출** 형태로 활용합니다.

### 3.1 StrOutputParser 예제

**예제 시나리오:** 간단한 퀴즈 질문에 **한 단어로 답변**을 요구하고, StrOutputParser로 그 결과를 받아보겠습니다. 프롬프트는 한국어로 작성하며, "한 단어로 답하라"는 지침을 줘서 응답이 간결한 문자열이 되도록 유도합니다.

```python
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 출력 파서와 프롬프트 설정
output_parser = StrOutputParser()
prompt = PromptTemplate(
    template="질문: {question}\n한 단어로 답하세요.", 
    input_variables=["question"]
)

# 체인 구성: PromptTemplate | LLM | OutputParser
chain = prompt | llm | output_parser

# 테스트 실행 (.invoke)
query = {"question": "서울은 어느 나라의 수도인가?"}
result = chain.invoke(query)

print("원본 문자열 출력:", result)
print("출력 타입:", type(result))
print("응답 글자수:", len(result))
```

```python
# 출력 예시:
# 원본 문자열 출력: 대한민국
# 출력 타입: <class 'str'>
# 응답 글자수: 4
```

* `result` 변수에는 LLM의 답변이 \*\*문자열(String)\*\*로 저장됩니다. 여기서는 질문에 대한 정답인 `"대한민국"`이 한 단어로 반환되었고, 자료형은 `<class 'str'>`입니다.
* `StrOutputParser`를 사용함으로써 체인의 최종 출력이 **문자열 타입으로 보장**됩니다. (LangChain에서 별도의 OutputParser를 지정하지 않은 기본 LLMChain도 보통 문자열을 반환하지만, 특히 ChatOpenAI처럼 메시지 객체를 반환하는 모델의 경우 StrOutputParser를 쓰면 `.content` 내용만 추출해주는 효과가 있습니다.)

**후속 처리 예시:** 파서를 통해 얻은 문자열은 일반 파이썬 문자열로 다루면 됩니다. 예를 들어 위 코드에서 `len(result)`로 글자수를 구하거나, `result == "대한민국"`처럼 다른 문자열과 비교/검증할 수 있습니다. 이처럼 StrOutputParser는 **자유도가 가장 높지만 구조화는 안 된** 형태의 출력으로, 간단한 응답이나 자유 텍스트가 필요한 경우 사용합니다.

> **Note:** StrOutputParser는 출력에 별도의 형식 제약을 걸지 않기 때문에, LLM이 프롬프트를 제대로 따르지 않으면 예상과 다른 형식의 답변이 나올 수 있습니다. 따라서 **한 단어로 답하세요**와 같은 지침을 프롬프트에 포함하여 모델 출력을 제어하는 것이 중요합니다. (StrOutputParser 자체는 포맷 강제를 하지 않습니다.)

## 4. CommaSeparatedListOutputParser – 콤마 구분 리스트 파서

**CommaSeparatedListOutputParser**는 LLM 출력이 `**콤마(,)로 구분된 항목들**`로 이루어져 있다고 가정하고 이를 파싱하여 \*\*Python 리스트(list)\*\*로 변환해주는 파서입니다. 주로 "여러 개의 항목 나열" 같은 요청에 사용되며, 모델이 적절히 콤마로 항목을 구분하도록 프롬프트를 구성하면, 편리하게 결과를 리스트로 얻을 수 있습니다.

**주요 특징:**

* 응답을 쉼표로 구분된 문자열로 출력하도록 모델에게 지시하고, 그 출력 문자열을 `.split(",")` 등의 방식으로 분리하여 문자열 리스트 `List[str]`로 반환합니다.
* `get_format_instructions()` 메서드가 구현되어 있어, 프롬프트에 삽입할 수 있는 **형식 안내문**을 제공합니다. 기본적으로 `"Your response should be a list of comma separated values, eg: \`foo, bar, baz\`"\`와 같은 영문 지침이 반환됩니다.
* 형식 안내문을 사용하면 모델이 **포맷을 준수**하는 답변을 하도록 유도할 수 있습니다. (물론 한국어 수업이므로, 안내문을 직접 한글로 작성하는 방법도 있습니다.)

### 4.1 CommaSeparatedListOutputParser 예제

**예제 시나리오:** "아이스크림 맛"에 대한 다섯 가지 종류를 나열하도록 모델에게 요청해보겠습니다. 출력은 쉼표로 구분된 한 줄 목록이어야 하며, 이를 CommaSeparatedListOutputParser로 파싱하여 파이썬 리스트로 얻습니다.

우선 출력 파서의 `get_format_instructions()`를 사용해 어떤 지침을 줄지 확인해보고, 필요에 따라 한글로 수정해보겠습니다:

```python
from langchain_core.output_parsers import CommaSeparatedListOutputParser

cs_list_parser = CommaSeparatedListOutputParser()
format_instructions = cs_list_parser.get_format_instructions()
print("기본 형식 지침:\n", format_instructions)
```

```python
# 기본 형식 지침 (영문 예시 출력):
# Your response should be a list of comma separated values, eg: `foo, bar, baz`
```

위와 같이 영문으로 제공되는데, 한국어 프롬프트에 이 문장을 섞어도 모델은 이해할 수 있지만, **모든 프롬프트를 한국어로 일관**하기 위해 이 내용을 참고하여 한글 형식 지침을 직접 작성하겠습니다.

```python
format_instructions_ko = "응답은 쉼표로 구분된 값의 목록 형태로 출력하세요. 예: `바나나, 사과, 오렌지`"
prompt = PromptTemplate(
    template="다음 {item} 5가지를 나열하세요.\n{format_instructions}",
    input_variables=["item"],
    partial_variables={"format_instructions": format_instructions_ko}
)

chain = prompt | llm | cs_list_parser

# 체인을 실행하여 결과 얻기
input_data = {"item": "아이스크림 맛"}
result_list = chain.invoke(input_data)

print("출력 값:", result_list)
print("출력 타입:", type(result_list))
print("항목 수:", len(result_list))
print("첫 번째 항목:", result_list[0])
```

```python
# 출력 예시:
# 출력 값: ['초콜릿', '바닐라', '딸기', '녹차', '민트 초코']
# 출력 타입: <class 'list'>
# 항목 수: 5
# 첫 번째 항목: 초콜릿
```

* `CommaSeparatedListOutputParser` 덕분에 `result_list`는 파이썬 리스트 객체로 반환되었습니다. 실제 출력 예시를 보면 `['초콜릿', '바닐라', '딸기', '녹차', '민트 초코']`처럼 **문자열 요소 5개를 갖는 리스트**임을 확인할 수 있습니다.
* 리스트의 길이 `len(result_list)`는 5이며, `result_list[0]`으로 첫 번째 요소 `"초콜릿"`에 바로 접근할 수 있습니다. 이처럼 LLM의 텍스트 나열 결과를 바로 파이썬 리스트로 활용할 수 있게 되었습니다.

**후속 처리 예시:** 리스트 형태의 출력은 순회(iteration)하거나, 각 요소를 개별적으로 사용하기 좋습니다. 예를 들어 위 결과 리스트에서 아이스크림 맛 하나하나에 대해 추가 정보를 검색하거나, 필요한 경우 정렬 `sorted(result_list)`을 할 수도 있습니다. 또 리스트의 요소 개수를 세어서 모델이 요구한 개수를 제대로 맞췄는지 **검증**할 수도 있습니다. (예: 5개 요청에 5개를 답했는지 확인)

> **Note:** CommaSeparatedListOutputParser는 응답을 단순히 `text.split(",")` 방식으로 처리하기 때문에, **모델 출력에 콤마가 잘못 사용되면 예상치 못한 파싱 결과**가 나올 수 있습니다. 예를 들어 항목 자체에 콤마가 포함되어 있는 경우(문장이나 수치에 comma가 들어간 경우 등) 제대로 분리가 안될 수 있습니다. 또한 모델이 쉼표 대신 줄바꿈으로 목록을 출력하면 파싱 결과가 한 개의 항목이 될 수도 있습니다. 따라서:
>
> * 프롬프트에서 **쉼표로 구분**하라는 지침을 명확히 하고, 예시도 제시하는 것이 좋습니다.
> * 필요하면 응답을 받은 후 추가로 `result_list = [s.strip() for s in result_list]`처럼 공백을 제거하거나 빈 문자열을 걸러주는 후처리를 할 수 있습니다.

## 5. JsonOutputParser – JSON 출력 파서

**JsonOutputParser**는 LLM의 출력을 **JSON 형식**으로 간주하여 파싱해주는 출력 파서입니다. 모델이 `{ ... }` 형태의 JSON 문자열을 반환하면, 이를 Python의 딕셔너리(dict) 또는 리스트로 변환해주어 **구조화된 데이터**로 다룰 수 있게 합니다. JSON은 키-값으로 정보를 조직할 수 있으므로, 복잡한 응답을 체계화하거나 LLM과 다른 프로그램 간의 데이터 교환 포맷으로 유용합니다.

**주요 특징:**

* 모델 출력 문자열을 JSON으로 **파싱하여 Python 객체**로 반환합니다. 일반적으로 `{ }`로 감싸진 객체는 `dict`로, `[ ]` 배열은 `list`로 로드됩니다. 내부에 숫자, 문자열 등이 타입에 맞게 매핑됩니다.
* `get_format_instructions()` 메서드로 **JSON 형태로 답하라는 지침**을 얻을 수 있습니다. 추가로 특정 **Pydantic 모델**을 제공하여 예상 필드/스키마를 지정할 수도 있습니다 (optional).
* JSON 파싱에 실패할 경우 `OutputParserException`을 발생시킵니다. 예컨대 모델 출력이 JSON 형식에 어긋나면 에러로 처리되므로, 엄격한 구조화에 활용할 수 있지만 그만큼 **모델이 정확히 포맷을 지키도록** 프롬프트를 잘 설계해야 합니다.

### 5.1 JsonOutputParser 예제

**예제 시나리오:** 사용자에게 **영화 추천**을 하나 해주는 모델 응답을 생각해봅시다. 영화의 제목, 출시년도, 장르를 JSON 객체로 반환하도록 프롬프트를 구성하고, JsonOutputParser로 이를 파싱해보겠습니다.

모델에게 JSON의 키 이름과 형식을 정확히 지키도록 지시하기 위해, 프롬프트에 **JSON 예시나 형식 설명**을 넣겠습니다. 또한 "추가 설명은 하지 말 것"을 강조하여 순수 JSON만 출력하게 유도합니다.

```python
from langchain_core.output_parsers import JsonOutputParser

json_parser = JsonOutputParser()

# 프롬프트 템플릿 정의
movie_prompt = PromptTemplate(
    template=(
        "다음 사용자 취향에 맞는 영화를 한 편 추천해주세요.\n"
        "취향: {preference}\n\n"
        "아래 형식의 JSON으로만 답변하세요 (추가 설명 금지):\n"
        '{{ "title": "<영화 제목>", "year": <출시년도>, "genre": "<장르>" }}'
    ),
    input_variables=["preference"]
)

chain = movie_prompt | llm | json_parser

# 입력 예: SF 장르를 좋아하는 경우
user_input = {"preference": "SF (공상 과학)"}
result_dict = chain.invoke(user_input)

print("출력 타입:", type(result_dict))
print("JSON 파싱 결과:\n", result_dict)
print("제목:", result_dict.get("title"))
```

```python
# 출력 예시:
# 출력 타입: <class 'dict'>
# JSON 파싱 결과:
#  {'title': '인터스텔라', 'year': 2014, 'genre': 'SF'}
# 제목: 인터스텔라
```

* 모델의 답변이 `"title": ..., "year": ..., "genre": ...` 키를 갖는 JSON 문자열로 생성되었고, JsonOutputParser에 의해 Python 딕셔너리로 변환되었습니다. `result_dict`의 타입은 `<class 'dict'>`이며, `result_dict["title"]` 또는 `result_dict.get("title")`로 영화 제목을 쉽게 추출할 수 있습니다.
* 프롬프트에 JSON **예시 템플릿**을 명시함으로써, 모델이 가능하면 정확한 형식을 따르도록 했습니다. 출력 결과를 보면 중괄호, 따옴표, 콜론 등의 **JSON 구문이 정확**하게 지켜졌음을 확인할 수 있습니다. (따옴표는 모두 쌍따옴표여야만 유효한 JSON입니다.)
* JsonOutputParser는 내부적으로 `json.loads()`와 유사한 파싱을 수행하므로, **모델 출력에 조금이라도 형식 오류가 있다면 예외**를 일으킵니다. 이 점을 이용해 LLM의 응답을 검증하거나, 잘못된 형식의 응답은 걸러내고 재시도하도록 만드는 전략도 가능합니다.

**후속 처리 예시:** JSON으로 받은 데이터는 곧바로 코드에서 활용할 수 있습니다. 예를 들어 위 결과의 `"year": 2014` 값을 사용해 해당 연도의 다른 영화를 찾아본다든지, `"genre": "SF"`를 활용해 SF 카테고리에 분류하거나 할 수 있겠습니다. 또한 복잡한 응답 (예: 다중 항목 추천 등)도 JSON 배열이나 중첩 구조로 표현하면 체계적으로 다룰 수 있습니다.

> **Note:** JSON 출력의 **실패 사례**로는, 모델이 형식을 어겨서 `"title": "인터스텔라", 'year': 2014, ...`처럼 잘못된 따옴표를 쓴다거나, `{}` 밖에 다른 설명을 덧붙이는 경우 등이 있습니다. 이럴 때 JsonOutputParser는 예외를 발생시키며, 다음 섹션에서 이러한 상황에 대한 오류 처리 및 디버깅 방법을 다루겠습니다. 또한 보다 엄격한 스키마 준수를 위해 **PydanticOutputParser**를 사용하는 방법도 있지만 (Pydantic 모델을 전달하여 추가 검증), 이번 수업에서는 기본 JsonOutputParser 범위 내에서 다룹니다.

## 6. 출력 파서 활용 팁: 오류 처리 및 디버깅

LLM의 출력이 항상 완벽하게 형식에 맞춰 떨어지는 것은 아닙니다. 출력 파서를 사용할 때 **파싱 오류**가 발생할 수 있고, 모델이 지시를 어겨 엉뚱한 포맷으로 답변할 수도 있습니다. 이러한 상황에 대비해 오류를 처리하고 디버깅하는 방법을 알아두어야 합니다.

### 6.1 OutputParserException 및 예외 처리

LangChain의 출력 파서들은 대체로 파싱에 실패하면 `OutputParserException`을 발생시킵니다. 예를 들어 JsonOutputParser는 **JSON이 유효하지 않을 경우 예외**를 던지도록 구현되어 있습니다. 이러한 예외를 적절히 처리하면, 잘못된 출력에 대응하거나 재시도를 요청하는 로직을 작성할 수 있습니다.

**예외 처리 예시:** JsonOutputParser를 이용하면서 발생할 수 있는 오류를 가정해 보겠습니다. (고의로 모델에게 형식을 지키지 않도록 유도하여 에러를 내는 식으로 실험할 수도 있습니다.)

```python
from langchain.schema import OutputParserException

# 프롬프트를 약간 변형하여 실수 유도: JSON 키에 따옴표를 쓰지 말라고 해보기 (잘못된 제안)
bad_prompt = PromptTemplate(
    template=(
        "다음 질문에 답하고 JSON으로 출력하세요 (키에 따옴표는 빼도 됨):\n"
        "{question}"
    ),
    input_variables=["question"]
)
chain_bad = bad_prompt | llm | json_parser

try:
    bad_result = chain_bad.invoke({"question": "세계에서 가장 높은 산은?"})
    print("파싱 결과:", bad_result)
except OutputParserException as e:
    print("OutputParserException 발생! 오류 메시지:", str(e))
```

```python
# (예시) 출력:
# OutputParserException 발생! 오류 메시지: Could not parse LLM output into JSON.
```

위 시나리오는 인위적인 예지만, `키에 따옴표는 빼도 됨`이라는 잘못된 지시로 인해 모델이 JSON 표준에 어긋나는 출력 (`{ title: 에베레스트, ... }`)을 생성했고, JsonOutputParser가 이를 파싱하지 못해 예외가 발생한 모습입니다.

이 때 `OutputParserException`을 캐치하여:

* 오류 메시지 (`e` 객체의 문자열 표현)를 기록하거나 사용자에게 알려줄 수 있습니다.
* `chain_bad` 대신 **출력 파서가 없는 체인** `(prompt | llm)`을 이용해 **원본 LLM 응답**을 확보할 수 있습니다. 위 코드에서는 raw\_response로 출력 내용을 확인했습니다. 이를 통해 어디서 형식이 틀어졌는지 분석할 수 있습니다.
* 필요한 경우 **다시 프롬프트를 개선하거나** 모델에게 재시도를 요청할 수 있습니다. 예를 들어, 위 예시에서는 키에 따옴표를 빼라는 지시가 잘못이므로 제거하고 다시 시도하면 될 것입니다.

**OutputParserException 외 예외:** CommaSeparatedListOutputParser나 StrOutputParser는 단순한 구현이어서, 어지간해선 예외를 던지지 않고 결과를 내려고 할 것입니다. (예: 쉼표 없는 문자열을 CommaSeparatedListOutputParser로 파싱하면 길이 1의 리스트로 반환). 하지만 원하는 구조와 다를 경우에 대비해, **결과의 타입이나 값 검증을 직접 수행**하는 것도 권장됩니다. 예를 들어 리스트를 기대했는데 타입이 리스트가 아니면 문제가 있는 것이므로 `isinstance(result, list)` 검사를 하는 식입니다.

## 7. 정리 및 추가 논의

이상으로 LangChain의 주요 OutputParser인 **StrOutputParser, CommaSeparatedListOutputParser, JsonOutputParser**에 대해 살펴보았습니다. 요약하면:

* **StrOutputParser**: LLM의 응답을 가공 없이 문자열로 반환합니다. 자유형식 응답이나 간단한 Q\&A에 사용합니다.
* **CommaSeparatedListOutputParser**: 응답을 쉼표로 구분된 문자열로 생성하도록 하고, 이를 자동으로 리스트로 변환합니다. 목록형 출력에 유용합니다.
* **JsonOutputParser**: JSON 형식의 응답을 파싱하여 딕셔너리/리스트 등 구조화된 Python 객체로 반환합니다. 복잡한 정보도 체계적으로 다룰 수 있게 해줍니다.

출력 파서를 사용함으로써 **LLM의 출력 내용을 구조화**하고, 이후 단계에서 그 결과를 프로그래밍적으로 쉽게 활용하거나 검증할 수 있게 됩니다. 이는 예측 불가능한 자연어 응답을 다루는 데 발생하는 위험성을 줄이고, LLM을 보다 **신뢰성 있는 컴포넌트**로 만드는 핵심 기법입니다. 특히 JSON처럼 엄격한 포맷은 외부 시스템과 연동하거나 DB에 저장할 때 큰 도움이 됩니다.

마지막으로, 실제 응용에서 출력 파서를 사용할 때는 **프롬프트 엔지니어링**도 함께 중요하다는 점을 기억해야 합니다. 모델에게 원하는 포맷을 예시와 함께 정확히 알려주고, 필요하면 한계 케이스에 대한 처리(예: 예외처리, 재시도 로직)를 구현하여 시스템의 견고함을 높이는 것이 좋습니다.

이번 수업의 내용을 토대로, 다양한 출력 파서와 맞춤형 파서를 실습해 보길 권장합니다. 필요에 따라 **RegexParser**나 **PydanticOutputParser**, 또는 직접 커스텀한 OutputParser를 만들어 특정 형식을 파싱하는 등 응용 범위를 넓혀갈 수 있습니다. 모두 수고하셨습니다!
