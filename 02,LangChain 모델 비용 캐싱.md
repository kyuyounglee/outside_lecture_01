# LangChain을 활용한 모델 사용, 비용 모니터링 및 캐싱 전략

LangChain 프레임워크와 OpenAI GPT-4o-mini 모델을 활용하여 LLM(대형 언어 모델)을 효과적으로 호출하고, 토큰 사용량 기반의 과금을 모니터링하며, 응답 결과를 캐싱(caching)하는 전략에 대해 학습합니다.

## GPT-4o-mini 모델 호출 방식 복습 (ChatOpenAI)

**GPT-4o-mini**는 OpenAI가 GPT-4 Omni(GPT-4o)의 경량 버전으로 출시한 모델로, 뛰어난 성능을 유지하면서도 비용을 크게 절감할 수 있는 모델입니다. GPT-4o-mini는 텍스트 입력과 이미지 입력을 모두 받아들이고 텍스트로 출력을 생성할 수 있으며, GPT-3.5 Turbo보다도 **성능이 우수하면서 비용이 60% 저렴**하여(ChatGPT 인터페이스에서 GPT-3.5를 대체함) 기업이나 개발자가 **대량의 API 호출을 할 때 유용**합니다.

이 섹션에서는 LangChain을 사용하여 GPT-4o-mini 모델을 호출하는 방법을 복습합니다. LangChain v0.3.x부터 OpenAI 지원이 별도 패키지로 분리되었으므로, 우선 필요한 라이브러리를 설치하고 API 키를 설정해야 합니다.

* **필요 패키지 설치:** LangChain 및 OpenAI 연동을 위해 `langchain-openai` 패키지가 필요하며, 토큰 사용량 추적과 캐싱을 위한 `langchain-community`도 설치합니다. 또한 환경 변수 관리를 위해 `python-dotenv`을 설치합니다.

```bash
pip install langchain-openai langchain-community python-dotenv openai
```

* **API 키 설정 (.env 파일):** OpenAI API를 사용하려면 API 키가 필요합니다. 프로젝트 루트 디렉터리에 `.env` 파일을 생성하고, 다음과 같이 키를 설정하세요 (실제 키 값은 `<...>` 부분에 입력):

```text
OPENAI_API_KEY=<발급받은 OpenAI API 키>
```

* **환경 변수 로드:** Python 코드에서 `python-dotenv`를 이용해 `.env` 파일의 내용을 불러올 수 있습니다. Jupyter나 VS Code에서 다음 코드를 실행하세요:

```python
from dotenv import load_dotenv
load_dotenv()  # .env 파일에 설정된 환경변수를 불러옵니다.
```

이제 LangChain을 통해 GPT-4o-mini 모델을 호출해보겠습니다. LangChain의 `ChatOpenAI` 클래스를 사용하면 OpenAI의 Chat Completion API를 간편하게 호출할 수 있습니다. 다음은 ChatOpenAI를 초기화하고 간단한 프롬프트로 답변을 얻는 코드 예제입니다:

```python
from langchain_openai import ChatOpenAI

# GPT-4o-mini 모델 초기화 (온도 설정 가능, 0이면 출력의 무작위성 최소화)
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)

# LLM에 간단한 질문을 보내고 응답을 받아봅니다.
prompt = "LangChain에 대해 한 문장으로 설명해줘."
result = llm.invoke(prompt)  # 프롬프트 문자열을 바로 전달하여 호출

print("질문:", prompt)
print("답변:", result.content)
```

위 코드에서는 `ChatOpenAI` 객체를 생성할 때 모델 이름을 `"gpt-4o-mini"`로 지정했습니다. `temperature` 파라미터로 생성물의 무작위 정도를 조절할 수 있으며 (0에 가까울수록 deterministic한 응답), 그 외에 `max_tokens`, `streaming` 등의 옵션을 사용할 수도 있습니다. `llm.invoke(...)` 메서드는 주어진 문자열을 **사용자 메시지**로 간주하여 모델의 응답을 바로 반환합니다. 반환값 `result`는 LangChain의 `AIMessage` 객체로, `.content` 속성에 모델의 답변 텍스트를 담고 있습니다.

예를 들어 위 코드를 실행하면 다음과 같은 출력이 나올 수 있습니다:

```python
질문: LangChain에 대해 한 문장으로 설명해줘.
답변: LangChain은 대형 언어 모델과 외부 도구들을 결합하여 응용 프로그램을 쉽게 개발할 수 있게 해주는 프레임워크입니다.
```

> **참고:** LangChain v0.3.x에서는 `ChatOpenAI`를 비롯한 통합 클래스들이 별도 패키지 (`langchain_openai`)로 제공됩니다. 따라서 `langchain.chat_models import ChatOpenAI` 대신 `from langchain_openai import ChatOpenAI` 형태로 임포트해야 합니다. 또한, OpenAI API 호출 시 \*\*환경 변수 `OPENAI_API_KEY`\*\*를 자동으로 참조하므로, `.env`에 키를 넣고 `load_dotenv()`로 불러왔다면 코드에서 키를 직접 지정할 필요가 없습니다.

## OpenAI API 토큰 기반 과금 구조 요약

OpenAI의 API 이용 요금은 \*\*프롬프트 토큰(입력)\*\*과 **완료 토큰(출력)** 수에 따라 책정됩니다. 즉, 사용자가 보낸 질문이나 시스템/맥락(prompt)으로 보낸 텍스트에 해당하는 **입력 토큰 수**와 모델이 생성하여 응답한 **출력 토큰 수**를 각각 계산하여 비용을 청구합니다. 모델마다 1토큰당 가격이 다르게 책정되어 있는데, 일반적으로 **더 강력한 모델일수록 단가가 높습니다**.

GPT-4o-mini를 비롯한 주요 모델들의 토큰 비용 구조를 살펴보면 다음과 같습니다:

* **GPT-4o-mini:** 100만 개의 입력 토큰당 **\$0.15**, 출력 토큰당 **\$0.60**의 비용이 발생합니다. 토큰 1개 기준으로 환산하면 입력 토큰은 약 \$0.00000015, 출력 토큰은 \$0.0000006 정도입니다. 매우 저렴한 비용으로, **GPT-3.5 Turbo 대비 약 60% 낮은 가격**에 제공됩니다. 작은 비용이지만 대량의 토큰을 사용하면 누적되어 비용이 올라갈 수 있습니다.
* **GPT-4 Omni (GPT-4o):** GPT-4o-mini의 상위 모델로, 100만 개의 입력 토큰당 **\$2.50**, 출력 토큰당 **\$10.00**로 가격이 훨씬 높습니다. 성능은 뛰어나지만 과금 단가가 높기 때문에 **복잡한 작업에만 제한적으로 사용**하거나, 가능한 한 필요한 부분에만 활용하는 전략이 요구됩니다.
* **GPT-3.5 Turbo:** (참고) 이전 세대 모델인 GPT-3.5 Turbo의 경우 100만 개 토큰당 수백만 원 수준의 비용이었으며, GPT-4o-mini 출시 이후 ChatGPT 서비스에서는 GPT-3.5 Turbo를 GPT-4o-mini가 대체하였습니다. GPT-4o-mini는 GPT-3.5 대비 **성능 향상과 비용 절감**을 모두 이뤄냈습니다.

요약하면, **OpenAI API는 요청(Request)과 응답(Response)에 사용된 토큰 양에 비례하여 비용**이 부과됩니다. 따라서 **프롬프트가 길거나 출력이 많은** 작업일수록 비용이 올라가며, 모델 종류에 따라 그 증가 폭이 커집니다. 예를 들어 GPT-4o-mini 모델에 1000토큰을 입력하고 1000토큰을 출력받았다면, 입력 비용 \$0.00015 + 출력 비용 \$0.0006 = 총 \$0.00075 정도가 청구됩니다. 반면 동일한 작업을 GPT-4o (상위 모델)로 수행했다면 **약 \$0.0125**가 들며, GPT-4(8k 버전) 같은 최상위 모델에서는 이보다 훨씬 높은 단가로 계산됩니다.

> **참고:** OpenAI의 과금 구조는 기본적으로 1,000토큰 단위(\$/1K tokens)로 공시되지만, 위에서는 이해를 돕기 위해 **1,000,000토큰(백만 단위)** 기준으로 서술했습니다. 실제 청구는 사용한 토큰 수에 비례하여 소수점 단위까지 산정됩니다.

## OpenAI API 사용 시 비용 발생 주의사항

**실제 OpenAI API를 호출하면 해당 계정에 실시간으로 과금이 발생**합니다. 실습이나 개발 단계에서 API를 빈번히 호출할 경우, 누적된 토큰 사용량만큼 비용이 청구되므로 주의해야 합니다. 몇 가지 유의사항을 정리하면 다음과 같습니다:

* **무료 용량 및 요금 확인:** OpenAI는 신규 가입자에게 일정량의 무료 크레딧을 제공하기도 하지만 한시적일 수 있습니다. **OpenAI 대시보드**의 Usage(사용량) 페이지에서 현재 사용량과 남은 무료 크레딧, 그리고 과금 상태를 수시로 확인하세요.
* **환경 설정으로 오남용 방지:** 개발 중 실수로 무한 루프나 과도한 API 호출이 발생하면 큰 비용이 나올 수 있습니다. 이를 방지하기 위해 API 키를 코드상에 쉽게 켜고 끌 수 있는 구조로 만들어두세요. 예를 들어 `.env` 파일에서 API 키를 지우거나, `OPENAI_API_KEY`를 임시로 다른 값으로 변경하면 API 호출이 실패하도록 하여 실수로 인한 추가 호출을 막을 수 있습니다. 또는 OpenAI 설정에서 \*\*사용량 한도(cap)\*\*를 설정해두면 일정 금액 이상 청구되지 않도록 제한할 수 있습니다.
* **소규모 테스트:** 비용이 우려된다면 초기에 **짧은 프롬프트와 낮은 토큰 제한**으로 테스트하고, 정상 동작을 확인한 후 점차 실제 입력으로 늘리는 것이 좋습니다. 또한 ChatGPT 웹 인터페이스(챗GPT Plus 구독 등)를 사용한 간단한 실험으로 응답 경향을 파악하고, 코드에서는 최소한의 호출만 수행하는 방식으로도 비용을 줄일 수 있습니다.
* **모델 선택:** 반드시 GPT-4급의 성능이 필요하지 않은 작업에는 **GPT-4o-mini나 GPT-3.5**와 같은 **저비용 모델을 우선 사용**해보는 것을 권장합니다. GPT-4o-mini는 특히 **동일한 작업을 GPT-4o로 실행하는 경우 대비 입력 토큰 비용은 약 1/17, 출력 토큰 비용은 1/16 수준**으로 매우 경제적입니다.
* **로그와 모니터링 활용:** 아래에서 다룰 **토큰 사용량 추적** 및 **LangSmith 대시보드**를 적극 활용하세요. 이를 통해 어느 부분에서 토큰을 많이 쓰는지 파악하고, 비효율적인 호출을 최적화할 수 있습니다. 예를 들어, 불필요하게 같은 질문을 여러 번 보내고 있지는 않은지, 한 번의 응답으로 충분한 것을 나눠서 호출하고 있지는 않은지 점검할 수 있습니다.

결론적으로, **OpenAI API 사용 = 실시간 비용 발생**임을 항상 염두에 두고, **필요한 최소한으로 호출**하는 습관과 모니터링 체계를 갖추는 것이 중요합니다.

## LangChain으로 토큰 사용량 추적하기

운영 단계에서 **토큰 사용량을 추적하여 비용을 계산**하는 것은 매우 중요합니다. 이번에는 LangChain을 활용해 각 호출마다 사용된 토큰 수를 추적하고 표시하는 방법을 알아보겠습니다. LangChain v0.3.x에는 토큰 사용량을 얻기 위한 몇 가지 편의 기능이 있습니다.

### 1. 응답 객체의 Usage Metadata 활용

OpenAI의 Chat Completion API는 응답과 함께 해당 호출의 토큰 사용량 정보를 제공합니다. LangChain의 `ChatOpenAI`를 통해 얻은 응답 객체(`AIMessage`)에는 이 정보가 \*\*`usage_metadata`\*\*로 저장됩니다. `AIMessage.usage_metadata` 딕셔너리는 `"input_tokens"`, `"output_tokens"`, `"total_tokens"` 등의 키로 토큰 수를 담고 있습니다.

앞서 GPT-4o-mini 모델을 호출한 `result` 객체에 대해 `usage_metadata`를 출력해보겠습니다:

```python
print("토큰 사용 정보:", result.usage_metadata)
# 출력 예시: 토큰 사용 정보: {'input_tokens': 15, 'output_tokens': 32, 'total_tokens': 47}
```

위 예시에서, 프롬프트에 15토큰, 모델의 응답에 32토큰이 사용되어 총 47토큰이 소비되었음을 보여줍니다. `usage_metadata`는 OpenAI가 응답과 함께 준 메타데이터를 그대로 담은 것으로, **해당 호출 1회분의 토큰 사용량**을 알려줍니다. (캐시를 사용했거나 멀티모달 입력 등 특수한 상황에서는 `cached_tokens` 등의 항목이 포함될 수도 있습니다.)

### 2. 콜백을 통한 누적 토큰 추적 (`get_openai_callback`)

여러 번의 LLM 호출에 걸쳐 **토큰 사용량을 누적 추적**하고 싶다면 LangChain의 **콜백(callback)** 기능을 이용할 수 있습니다. `langchain_community.callbacks` 모듈의 `get_openai_callback` 함수를 사용하면 OpenAI 모델 호출에 특화된 콜백 핸들러를 **context manager** 형태로 얻을 수 있습니다. 해당 콜백은 블록 내부에서 발생하는 모든 OpenAI API 호출의 토큰 사용량을 합산해줍니다.

다음은 두 번의 연속된 모델 호출에 대해 `get_openai_callback`으로 토큰 사용량을 추적하는 예제입니다:

```python
from langchain_community.callbacks import get_openai_callback

with get_openai_callback() as cb:
    # 첫 번째 호출
    res1 = llm.invoke("서울의 오늘 날씨는 어떤지 알려줘.")  # 프롬프트 예시
    print("응답1:", res1.content[:30], "...")  # 응답 일부 출력 (길 경우 잘라내기)
    # 두 번째 호출
    res2 = llm.invoke("파이썬으로 리스트 만드는 방법 예시를 알려줘.")
    print("응답2:", res2.content[:30], "...")

# 콜백 cb에는 블록 내 전체 토큰 사용량이 누적되어 있습니다.
print(f"총 토큰 수: {cb.total_tokens}")
print(f"프롬프트 토큰: {cb.prompt_tokens}")
print(f"응답 토큰: {cb.completion_tokens}")
print(f"총 비용 (USD): ${cb.total_cost:.6f}")
```

위 코드에서 `with get_openai_callback() as cb:` 블록을 시작하면 `cb` 객체가 콜백 핸들러로 작동합니다. 블록 내부에서 일어난 모든 OpenAI 모델 호출(`llm.invoke`)은 자동으로 `cb`에 토큰 집계 정보를 전달합니다. 블록을 벗어나면 `cb` 객체에 `total_tokens`, `prompt_tokens`, `completion_tokens`, `total_cost` 속성이 채워지며, 이를 출력하여 전체 사용량과 비용을 확인할 수 있습니다.

예를 들어 위 실습을 실행하면 다음과 같은 출력 결과를 얻을 수 있습니다 (예시 수치):

```python
응답1: 오늘 서울은 대체로 맑고 기온은 ... 
응답2: 파이썬에서 리스트를 만들려면 대괄호([])를 ... 

총 토큰 수:  ninety
프롬프트 토큰:  35
응답 토큰:  55
총 비용 (USD): $0.000009
```

위 가상의 결과에서 두 번의 요청에 걸쳐 총 90 tokens(프롬프트 합계 35 + 응답 합계 55)을 사용했고, GPT-4o-mini의 요율로 약 \$0.000009 달러의 비용이 발생했음을 보여줍니다. **참고로, `total_cost`는 LangChain이 사전 내장한 모델 단가를 기반으로 자동 계산된 값**입니다. (GPT-4o-mini의 입력 토큰 단가 \$0.15/백만, 출력 \$0.6/백만을 반영하여 계산되었음). 만약 GPT-4o-mini가 아닌 다른 모델을 사용하면 해당 모델의 공시 요율로 비용이 계산됩니다.

> **주의:** `get_openai_callback` 콜백은 기본적으로 OpenAI의 Chat Completion API (`ChatOpenAI` 등)에서 제공하는 토큰 usage 메타데이터를 활용합니다. 따라서 **스트리밍 모드로 응답을 받을 때**는 기본적으로 토큰 집계가 실시간으로 업데이트되지 않습니다. 만약 스트리밍 응답의 토큰 사용량을 추적하려면 `stream_usage=True` 옵션을 `ChatOpenAI`에 주거나(이 기능은 LangChain 0.3.11+에서 지원) 혹은 LangSmith 등의 추적 도구를 사용해야 합니다.

## LangChain LLM 응답 캐싱 소개 및 실습 (`InMemoryCache`, `SQLiteCache` 활용)

LLM API 호출은 상대적으로 **응답 속도가 느리고 비용이 발생**하기 때문에, 동일한 요청에 대해서는 한 번만 호출하고 결과를 저장해두었다가 재사용하는 **캐싱(caching)** 전략이 유용합니다. LangChain은 \*\*선택적 캐시 계층(Cache layer)\*\*을 제공하여, **반복되는 LLM 호출 결과를 저장**하고 나중에 같은 입력이 들어오면 실제로 모델을 호출하지 않고 캐시된 결과를 즉시 반환할 수 있습니다. 이를 통해 **불필요한 API 호출을 줄여 비용을 절감하고 응답 속도를 향상**시킬 수 있습니다.

### 1. In-Memory 캐시 (메모리 캐시) 사용

가장 간단한 형태는 \*\*메모리 내 캐시(InMemoryCache)\*\*를 사용하는 것입니다. 이는 현재 Python 프로세스의 메모리에 응답을 저장하므로, 어플리케이션 실행 중에만 유효하고 영구 저장되지 않습니다. 메모리 캐시를 설정하는 방법은 다음과 같습니다:

```python
from langchain_core.caches import InMemoryCache
from langchain_core.globals import set_llm_cache

# InMemoryCache 설정
set_llm_cache(InMemoryCache())
```

`set_llm_cache` 함수를 통해 전역 LLM 캐시를 설정하면, 이후 `llm.invoke` 등의 LLM 호출 시 LangChain이 내부적으로 캐시를 확인합니다. **같은 모델 객체(`llm`)에 대해 동일한 프롬프트**로 요청이 들어오면, 캐시된 결과가 있다면 API를 호출하지 않고 그 결과를 반환합니다. (프롬프트 문자열뿐 아니라 내부적으로 모델의 매개변수 설정까지 캐시 키에 일부 반영됩니다. 즉 temperature 등이 동일해야 동일한 결과로 간주합니다.)

실제로 캐시 사용 전후를 비교해보겠습니다. 다음은 캐시 설정 후 **같은 질문을 두 번 호출**하는 코드입니다:

```python
set_llm_cache(InMemoryCache())  # 캐시 활성화

query = "재미있는 과학자 유머 하나 알려줘."  # 동일한 질문을 두 번 보낼 예정
# 첫 번째 호출 (캐시에 없다면 API 호출 발생)
result1 = llm.invoke(query)
print("응답1:", result1.content)

# 두 번째 호출 (동일한 query, 캐시 히트 시 API 미호출)
result2 = llm.invoke(query)
print("응답2:", result2.content)
```

만약 캐시가 정상적으로 작동했다면, `result1`과 `result2`의 `.content`는 완전히 동일한 내용일 것이고, **두 번째 호출에서는 OpenAI API를 부르지 않았으므로 훨씬 빠르게 결과가 반환**됩니다.

**실행 시간 측정**을 통해 차이를 확인해볼 수도 있습니다:

```python
import time
# 첫 번째 호출 시간 측정
start = time.time(); llm.invoke(query); end = time.time()
print(f"첫 호출 시간: {end - start:.3f} 초")

# 두 번째 호출 시간 측정 (캐시 예상)
start = time.time(); llm.invoke(query); end = time.time()
print(f"두번째 호출 시간: {end - start:.6f} 초")
```

출력 예시:

```
첫 호출 시간: 0.912 초
두번째 호출 시간: 0.000280 초
```

위 결과에서 보듯, 처음 호출은 약 0.9초 걸린 반면 캐시된 두 번째 호출은 **0.00028초(280마이크로초)** 정도로 거의 즉시 완료되었습니다. LangChain 공식 문서에서도 "두 번째 호출은 훨씬 빨라진다"고 명시하고 있으며, 예시로 **첫 호출 1.11초 대비 두 번째 호출 270μs로 단축**된 결과를 보여줍니다. 또한 두 번째 호출에서는 OpenAI API를 사용하지 않으므로 **토큰 사용량이나 비용도 추가로 발생하지 않습니다**.

> **노트:** LangChain의 캐시는 **프롬프트 문자열이 완전히 동일할 경우에만** 매치됩니다. 유사하지만 공백 하나가 다르거나, 프롬프트 내에 현재 시간처럼 가변 값이 있으면 별도의 캐시 키로 인식되어 히트되지 않습니다. 또한 `temperature`와 같은 파라미터에 따라 출력 내용이 달라질 수 있는 경우에도, 현재 캐시는 이러한 확률적 출력을 그대로 저장하여 반환합니다. 따라서 캐시 사용시 `temperature=0` (결과가 deterministic하도록) 설정을 권장하며, 그렇지 않을 경우 캐시된 응답이 항상 최선이라는 보장이 없음을 인지해야 합니다. 필요에 따라 **캐시를 무효화하거나 갱신**하는 로직을 추가적으로 관리해야 할 수도 있습니다.

### 2. SQLite 캐시 (디스크 기반 캐시) 사용

메모리 캐시는 애플리케이션을 재시작하면 사라진다는 한계가 있습니다. **영구적으로 캐시를 저장**하고 여러 실행 간에도 공유하고 싶다면 **SQLiteCache**를 사용할 수 있습니다. SQLiteCache는 내부적으로 SQLite 데이터베이스 파일을 사용하여 캐시를 저장하므로, 앱을 종료하고 다시 실행해도 이전에 저장된 캐시를 불러올 수 있습니다.

SQLite 캐시 설정 방법은 아래와 같습니다:

```python
import os
from langchain_community.cache import SQLiteCache

# 기존 캐시 DB 삭제 (실습을 위해 .langchain.db 초기화)
if os.path.exists(".langchain.db"):
    os.remove(".langchain.db")

# SQLite 캐시 설정 (지정한 경로에 DB 파일 생성/사용)
set_llm_cache(SQLiteCache(database_path=".langchain.db"))
```

위 코드로 현재 경로에 `.langchain.db`라는 SQLite 데이터베이스 파일을 캐시로 사용하도록 설정했습니다. 이후 `llm.invoke` 호출 시 이 데이터베이스를 조회/갱신하여 캐시 기능을 수행합니다. 사용법은 InMemoryCache때와 동일하며, 캐시 데이터가 파일에 저장되므로 Python 프로세스가 종료되어도 남습니다.

SQLiteCache의 성능은 메모리보다는 약간 떨어지지만(디스크 I/O overhead), 그래도 API 호출을 하는 것보다는 훨씬 빠릅니다. LangChain 문서에서는 GPT-3.5 Turbo 모델로 **첫 호출 \~0.85초, 두 번째(캐시 히트) \~0.13초** 정도 걸린 예시를 보여줍니다. 수백 ms 내로 응답이 올 정도면, API를 부르는 것에 비해 충분한 이점이 있습니다.

이제 SQLite 캐시가 설정된 상태에서 위와 동일하게 질문을 두 번 호출하면, **첫 번째 호출 결과가 .langchain.db에 저장**되고 두 번째 호출은 이 DB에서 결과를 가져옵니다. 결과 내용은 메모리 캐시 때와 동일하게 일치할 것이며, 응답 속도도 향상됩니다. 한 가지 유의할 점은, 데이터베이스 파일 크기가 커질 수 있으므로 주기적으로 캐시를 정리하거나, 필요에 따라 키-값 쌍을 TTL(만료 시간)을 두고 관리하는 기능은 직접 구현해야 합니다 (기본 제공되진 않습니다).

### 3. 캐시 활용 시 고려사항

* 캐싱은 **동일한 입력에 대해 반복되는 호출이 많은 경우에만 유용**합니다. 사용자 입력이 매번 달라지는 챗봇 등에서는 캐시 효용이 낮을 수 있지만, 개발 중 실험 반복이나 데모 시연 등 동일한 질문/문장을 여러 번 실행하는 경우에는 캐시를 켜두면 편리합니다.
* **API 비용 절감** 효과: 캐시된 결과를 반환할 때는 OpenAI API 호출을 생략하므로 그만큼 비용이 들지 않습니다. 앞서 토큰 추적 콜백으로 감싸서 테스트해보면, 캐시를 켠 상태에서 동일 입력 두 번 호출 시 **두 번째 호출에서는 `cb.total_tokens` 증가가 없음을** 확인할 수 있습니다 (API 미호출이므로).
* **일관성**: 캐시는 과거의 응답을 반복해서 사용하기 때문에, 만약 시간에 민감한 질문(예: 날씨, 뉴스)이나 **업데이트된 정보가 필요한 질의**에는 부적절합니다. 캐시 때문에 오래전 응답이 재사용되면 잘못된 정보를 제공할 위험이 있습니다. 따라서 이러한 경우 캐시를 비활성화하거나, 프롬프트에 시간 정보를 넣어 항상 캐시 키가 달라지도록 처리해야 합니다.
* LangChain의 LLM 캐시는 **Beta 기능**으로 명시되어 있으며, 캐시 충돌이나 예상치 못한 동작 가능성도 완전히 배제할 순 없습니다. 그러나 일반적인 사용에는 큰 문제가 없으며, 공식 문서도 비용 절감과 속도 향상을 위해 캐시 활용을 권장하고 있습니다.

> **요약:** 캐싱을 사용하면 **동일한 LLM 요청에 대한 응답을 저장하여 재사용함으로써 시간과 비용을 아낄 수 있습니다**. LangChain은 메모리 캐시와 SQLite 캐시를 기본 제공하며, 필요에 따라 Redis나 기타 캐시 백엔드도 직접 구현하여 사용할 수 있습니다. 적절한 시나리오에서 캐싱을 적용하면 응답 속도가 크게 향상되고 비용도 줄어들지만, 항상 최신 응답이 필요한 맥락에서는 주의해서 사용해야 합니다.

---

이상으로 **LangChain을 활용한 모델 사용, 비용 모니터링 및 캐싱 전략**에 대해 알아보았습니다. 정리하면:

* LangChain `ChatOpenAI`로 GPT-4o-mini 등 OpenAI 모델을 쉽게 호출할 수 있고, `.env`와 `python-dotenv`를 활용해 API 키를 안전하게 관리합니다.
* OpenAI의 과금은 **토큰 기반**으로 이뤄지며, **입력(prompt)과 출력(completion)** 토큰에 각기 다른 요율이 적용됩니다. 모델별 단가 차이를 이해하고, **토큰 절약 = 비용 절약**임을 항상 염두에 둬야 합니다.
* LangChain의 **토큰 사용량 추적 도구** (usage metadata, `get_openai_callback`, `with_config` 등)를 통해 어느 요청에 몇 토큰이 사용되었는지 실시간으로 파악할 수 있습니다. 이는 곧 비용 모니터링으로 직결되어, 예산 관리에 도움을 줍니다.
* **LangSmith 플랫폼**과 연동하면 체계적인 **실행 추적 및 대시보드 모니터링**이 가능합니다. 대시보드에서 토큰 및 비용 데이터를 한눈에 보고, 문제 상황을 디버깅하거나 **ROI 분석**을 할 수 있습니다.
* **응답 캐싱 전략**을 사용하여 **중복 호출을 줄이고** 응답 시간을 극적으로 개선할 수 있습니다. LangChain 내장 캐시를 활용한 In-Memory 및 SQLite 캐시 실습을 통해, 캐싱이 어떻게 **1회 API 호출로 여러 번의 응답을 처리**할 수 있는지 확인했습니다.

실습한 내용들을 토대로, 실제 응용 프로그램 개발 시 **비용을 고려한 LLM 활용**에 많은 도움이 되길 바랍니다. 🚀

